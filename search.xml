<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Fast RCNN论文阅读笔记]]></title>
    <url>%2F2018%2F01%2F03%2FFast-RCNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文地址：https://arxiv.org/pdf/1504.08083.pdf项目地址：https://github.com/rbgirshick/fast-rcnn Fast RCNN是继2014年提出的RCNN之后，Ross Girshick在15年推出的，构思精巧，流程更为紧凑，大幅提升了目标检测的速度。同样使用最大规模网络，训练时间从84小时减为9.5小时，基于VGG16的Fast RCNN算法在训练速度上比RCNN快了将近9倍，比SPPnet快大概3倍；测试时间从47秒减少为0.32秒，测试速度比RCNN快了213倍，比SPPnet快了10倍。在VOC2012上的mAP在66%左右。 1.背景介绍Fast RCNN 是针对RCNN +SPPNet 的改进，改进的原因是：1.1.RCNN(1)训练是多级pipeline的。RCNN首先采用log代价函数在目标候选框上微调卷积网络，然后，将SVMs适用到卷积特征。SVMs取代了通过微调学习softmax分类器，充当了目标检测器的任务。在训练的第三阶段，学习检测框Bounding-box回归。(2)训练非常耗费空间和时间。对于SVM和Bounding-Box回归训练，针对每个图像的每个目标推荐的特征被提取，并被写到硬盘中。采用非常深的网络，如VGG16，这个过程大概会花费2.5个GPU-days，仅仅针对VOC2007训练集的5k图片。这些特征要求成百上千GB的存储空间。(3)目标检测非常的慢。在测试时，从每张测试图像提取的每个目标推荐的特征。在GPU上用VGG16来检测一张图片需要花费47s。由此可以看出RCNN的问题所在，首先在提取完proposal之后，整个网络对提取到的RCNN中的所有的proposal都进行了整套的提取特征这些操作，这些操作是非常耗时，耗费空间的。事实上我们并不需要对每个proposal都进行CNN操作，只需要对原始的整张图片进行CNN操作即可，因为我们所提取到的proposal属于整张图片，因此对整张图片提取出feature map之后，再找出对应proposal在feature map中对应的区域，进行对比分类即可。第二个问题所在就是在框架中2-3过程中的对提取到的区域进行变形，我们知道CNN提取特征的过程对图像的大小并无要求，只是在提取完特征，进行全连接的时候才需要固定尺寸的特征，然后使用SVM等分类器进行分类操作，当然这两个问题在SPPNET中得到了很好的解决。1.2.SPPNet 引入原因：在RCNN中，使用完ss提取proposal之后，对每个proposal都进行了CNN提取特征+SVM分类。解决方法：因为region proposal都是图像的一部分，我们只需要对图像提一次卷积层特征，然后将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。更直白的讲就是SPP-NET代替卷积网络中最后一个pooling层，而且这pooling层是多scale的。SPP_net 源于微软2014年发布的论文《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》，主要实现基于空间金字塔的分类与检测，该方法可以大大提高目标检测的速度，相比RCNN有60倍的性能提升。 SPP net中Region Proposal仍然是在原始输入图像中选取的，不过是通过CNN映射到了feature map中的一片区域。空间金字塔池化的思想是：对卷积层的feature map上Region Proposal映射的区域分别划分成1×1，2×2，4×4的窗口（window），并在每个窗口内做max pooling，这样对于一个卷积核产生的feature map，就可以由SPP得到一个（1×1+2×2+4×4）维的特征向量。论文中采用的网络结构最后一层卷积层共有256个卷积核，所以最后会得到一个固定维度的特征向量（1×1+2×2+4×4）×256维），并用此特征向量作为全连接层的输入后做分类。相比于R-CNN，SPP net是使用原始图像作为CNN网络的输入来计算feature map（R-CNN中是每个Region Proposal都要经历一次CNN计算），这样就大大减少了计算量。另外，SPP net中Region Proposal仍然是通过选择性搜索等算法在输入图像中生成的，通过映射的方式得到feature map中对应的区域，并对Region Proposal在feature map中对应区域做空间金字塔池化。通过空间金字塔池化操作，对于任意尺寸的候选区域，经过SPP后都会得到固定长度的特征向量。SPP net的优点：任意尺寸输入，固定尺寸输出。2.本文的贡献Fast RCNN针对RCNN在训练时是multi-stage pipeline和训练的过程中很耗费时间空间的问题进行改进。它主要是将深度网络和后面的SVM分类两个阶段整合到一起，使用一个新的网络直接做分类和回归。主要做了以下改进:(1)最后一个卷积层后加了一个ROI pooling layer。ROI pooling layer首先可以将image中的ROI定位到feature map，然后是用一个单层的SPP layer将这个feature map patch池化为固定大小的feature之后再传入全连接层。(2).损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练。 3.Fast RCNN结构和训练Fast RCNN的网络输入是整张图片和一组object proposals。网络首先用conv和max pooling处理整张图片生成整张图片的feature map。然后对于每一个object proposal，使用region of interest（RoI）polling层从feature map中抽取一个固定长度的特征向量。接下来将每一个特征向量通过两个全连接层得到最终的4096维的feature。分别送到softmax中去做分类和bbox regressor中做位置的回归。值得注意的是它把分类和回归也放在了CNN网络中来做，而不是像RCNN那样独立开来 3.1ROI Pooling LayerRoI(region of interest)是候选框（region proposal）映射到最后一层卷积层生成的。由于候选框的大小不同，映射到最后一层卷积层的RoI区域的大小也不同，这样就会导致特征提取的维度不同。因此需要通过RoI pooling得到固定长度的特征，以便输入到后面的全连接层中。RoI pooling的对象是输入图像中产生的proposal在feature map上的映射区域RoI pooling是这样的，它的输入是不同大小的feature map，那么为了得到相同大小的feature，怎么做呢？举个栗子，这两个窗口虽然不一样大，但是我都给它分成9份，然后在每个小区域做max pooling，这样都能得到9维的向量，这就是ROI pooling。 3.2.Initializing from pre-trained networks 预训练过程对网络修改主要有三个方面： -最后一个max pooling layer被RoI pooling layer代替。目的是将每个RoI对应的feature map resize到固定的大小（对于VGG16来说H与W都是7）使其能够传送到接下来的全连接层进行训练。-最后一个全连接层和softmax层被改成两个自层，分别是softmax做分类和bbox regression做回归。-网络的输入包含两个部分：图片和这些图片对应的RoIs。 3.3.Fine-tuning for detection 使用BP算法训练网络是Fast R-CNN的重要能力，前面已经说过，SPP-net不能微调spp层之前的层，主要是因为当每一个训练样本来自于不同的图片时，经过SPP层的BP算法是很低效的（感受野太大）。这正是训练RCNN和SPPNet网络的方法，低效的部分是因为每个ROI可能具有非常大的感受野，通常跨越整个输入图像。由于反向传播必须处理整个感受野，训练输入很大（通常是整个图像）。提出了一种更有效的训练方法，利用训练期间的特征共享。 Fast R-CNN提出SGD mini_batch分层取样的方法：首先随机取样N张图片，然后每张图片取样R/N个RoIs e.g. N=2 and R=128 除了分层取样，还有一个就是FRCN在一次微调中联合优化softmax分类器和bbox回归，看似一步，实际包含了多任务损失（multi-task loss）、小批量取样（mini-batch sampling）、RoI pooling层的反向传播（backpropagation through RoI pooling layers）、SGD超参数（SGD hyperparameters）。分层采样在Fast RCNN网络训练中，随机梯度下降（SGD）的小批量是被分层采样的，首先采样N个图像，然后从每个图像采样R/N个 RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减N，就减少了小批量的计算。例如，当N=2和R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。更加精细的训练过程在微调阶段联合优化Softmax分类器和检测框回归，而不是分别在三个独立的阶段训练softmax分类器，SVM和回归器。Multi-task loss多任务一起训练自然就需要定义一个多任务的误差函数.softmax的输出 p = (p_0,...,p_k)代表了K+1类对应的概率.对每一个类k,bbox regressor的输出是 t^k = (t^k_x,t^k_y,t^k_w,t^k_h)这跟R-CNN是一样的.多任务误差函数由两个部分组成.第一部分是分类误差(ground truth类的log误差) L_{cls}(p,u) = -log p_u第二部分是定位误差(bbox regressor的误差) {\lambda}[u \geq1]L_{loc}(t^u,v)对于u=0的,代表背景的RoI,由于背景并不存在真实位置,所以这部分的定位误差被忽略.而对于u&gt;=1的其他类,使用如下误差 L_{loc}(t^u,v)= \sum_{i\in\{x,y,w,h\}} smooth_{L1}(t^u_i - v_i)其中 smooth_{L1}(x)= \begin{cases} 0.5x^2\ \ if|x|]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN算法笔记]]></title>
    <url>%2F2017%2F12%2F24%2FRCNN%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文地址：https://arxiv.org/pdf/1311.2524.pdf项目地址：https://github.com/rbgirshick/rcnnRCNN 可以说是采用深度学习来解决目标检测任务的开山之作。发表于2014年发表于计算机视觉三大顶会之一CVPR上，作者Ross Girshick多次在PASCAL VOC 的目标检测竞赛中获得很好的成绩，是继DPM方法多年平台期过后，取得显著效果的一篇文章。 RCNN的主要贡献是将CNN方法引入到了目标检测领域，大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路。然后由Ross Girshick、Kaiming He、Shaoqing Ren和一系列工作人员在此基础上进行拓展，出现了一系列的two stage的基于Region Proposal 采用深度学习解决目标检测问题的一系列算法：Fast RCNN、 Faster RCNN、PVA-Faster RCNN等，代表了当下目标检测方法的一个前沿分支。论文的主要贡献1.将卷积神经网络应用Region Proposal的策略，自底向上来进行定位目标和图像分割。2.当标注数据比较稀疏时，在有监督的数据集上训练到特定任务数据集上fine-tuning可以取得较好性能。效果比当时最好的DPM算法 mAP高20点，在VOC上取得了最好的性能。 论文主要特点（比较于传统方法）1.速度：经典的目标检测算法使用滑动窗口法依次判断所有可能的区域。本文则采用（Selective Search方法）预先提取一系列较可能的物体候选区域，之后在这些候选区域上采用CNN进行特征提取，判断目标。2.训练集：经典的目标检测算法在区域中提取人工设定的特征。本文采用深度学习进行特征提取。使用了两个数据库：一个较大的识别数据库（ImageNet ILSVC 2012），其标定了每张图片中个物体数据的类别。一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置，20类。本文使用识别库进行预训练得到CNN（有监督预训练），然后采用检测库进行参数调优训练，最后在检测库上评测。 RCNN 算法基本流程1.一张图像自底向上生成2k个左右候选区域2.对每个候选区域，使用深度学习网络进行特征提取3.特征送入每一类的SVM分类器，判别是否属于该类4.使用回归器精细修正候选框位置Region Proposal（候选区域生成）使用 Selective Search 方法从一张图像生成约2k个候选区域。基本思路：1.使用一种过分割手段，将图像分割成小区域2.查看现有小区域，合并可能性最高的两个区域。重复直到整张图像合并成一个区域位置。3.输出所有曾经存在过的区域，即候选区域。候选区域和后续步骤相对独立，可采用任意算法进行。合并规则优先合并以下四种区域：-颜色（颜色直方图）相近的-纹理（梯度直方图）相近的-合并后总面积小的-合并后，总面积在其BBOX中所占比例大的。 第三条，保证合并操作的尺度比较均匀，避免一个大区域陆续“吃掉”其他小区域。例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh。第四条，保证合并后形状规则。上述四条规则只涉及区域的颜色直方图、梯度直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。有监督预训练与无监督预训练 (1)无监督预训练(Unsupervised pre-training) 预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。 (2)有监督预训练(Supervised pre-training) 所谓的有监督预训练也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务时：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样相比于你直接采用随机初始化的方法，精度可以有很大的提高。 对于目标检测问题： 图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇论文采用了迁移学习的思想： 先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片分类训练。这个数据库有大量的标注数据，共包含了1000种类别物体，因此预训练阶段CNN模型的输出是1000个神经元（当然也直接可以采用Alexnet训练好的模型参数）。重叠度（IOU）: 物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示就是矩形框A、B的重叠面积占A、B并集的面积比例。非极大值抑制（NMS）： RCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框为做类别分类概率：就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于车辆的概率 分别为A、B、C、D、E、F。 (1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值; (2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。 (3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。 就这样一直重复，找到所有被保留下来的矩形框。 非极大值抑制（NMS）顾名思义就是抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。这里不讨论通用的NMS算法，而是用于在目标检测中用于提取分数最高的窗口的。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。 VOC物体检测任务相当于一个竞赛，里面包含了20个物体类别：PASCAL VOC2011 Example Images 还有一个背景，总共就相当于21个类别，因此一会设计fine-tuning CNN的时候，我们softmax分类输出层为21个神经元。总体思路再回顾：首先对每一个输入的图片产生近2000个不分种类的候选区域（region proposals），然后使用CNNs从每个候选框中提取一个固定长度的特征向量（4096维度），接着对每个取出的特征向量使用特定种类的线性SVM进行分类。也就是总个过程分为三个程序：a、找出候选框；b、利用CNN提取特征向量；c、利用SVM进行特征向量分类。候选框搜索阶段： 当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法： (1)各向异性缩放 这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227*227，如下图(D)所示；(2)各向同性缩放 因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案。有两种办法 A、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示; B、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示; 对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。 （备注：候选框的搜索策略作者也考虑过使用一个滑动窗口的方法，然而由于更深的网络，更大的输入图片和滑动步长，使得使用滑动窗口来定位的方法充满了挑战。） CNN特征提取阶段： 1、算法实现 a、网络结构设计阶段 网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ） 参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这里文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。网络优化求解时采用随机梯度下降法，学习率大小为0.001；C、fine-tuning阶段 （图片数据库： PASCAL VOC） 我们接着采用 selective search 搜索出来的候选框 （PASCAL VOC 数据库中的图片） 继续对上面预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景) (20 + 1bg = 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个事正样本、96个事负样本。关于正负样本问题： 一张照片我们得到了2000个候选框。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此在CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框（PASCAL VOC的图片都有人工标注）的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本）。（ 如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了） 疑惑点： CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，在训练的时候最后一层softmax就是分类层。那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？ 这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm，具体请看下文。 SVM训练、测试阶段 训练阶段： 这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到20004096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background) 。 排序，canny边界检测之后就得到了我们需要的bounding-box。 再回顾总结一下：整个系统分为三个部分：1.产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标2.一个大的卷积神经网络，对每个region产生一个固定长度的特征向量3.一系列特定类别的线性SVM分类器。位置精修： 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。测试阶段： 使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后再CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数，再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(then B-BoxRegression)。 （非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类）。作者提到花费在region propasals和提取特征的时间是13s/张-GPU和53s/张-CPU，可以看出时间还是很长的，不能够达到及时性。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNets论文阅读笔记]]></title>
    <url>%2F2017%2F12%2F23%2FMobileNets%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[移动和嵌入式设备视觉应用的高效卷积神经网络模型：MobileNets论文地址：https://arxiv.org/abs/1704.04861 摘要本文针对移动和嵌入式视觉设备提出一类被称作MobileNets的高效模型。MobileNets是一个使用深度可分解卷积（depthwise separable convolutions）来构建轻量级深度神经网络的精简结构（streamlined architecture，流线型结构 or 精简结构，倾向于后者）。本文引入了两个 简单的全局超参来有效权衡延迟（latency）和准确度（accuracy）。这些超参允许模型构建者根据具体问题的限制为他们的应用选择规模合适的模型。在资源和准确度的权衡条件下进行广泛的实验，与ImageNet分类任务的其它主流模型相比较，本文的模型显示出很好的性能。然后 ，本文在一系列的应用和用例下验证了其有效性，包括：目标检测、细粒度分类、人脸属性提取和 大规模地理定位。 1.引言 自从AlexNet在ILSVRC2012利用深度卷积神经网络赢得ImageNet挑战赛以来，卷积神经网络（CNN）在计算机视觉领域被广泛使用。这方面的应用主流趋势是采用更深、更复杂的网络实现更高的精度。但是，考虑到模型大小和速度。精度的提升并不一定使网络更加高效。在很多实际的应用场景，如机器人、自动驾驶和 增强现实，这些识别任务需要在计算资源限制的平台实时地运行。 本文提出了一个高效的网络结构和一组两个超参，用以构建较小的、低延迟模型，从而能比较好的满足移动和 嵌入式视觉应用的设计要求。 2.背景介绍在最近的文献中，关于建立小型高效的神经网络的研究日益增加。一般说来，这些方法可以归为两类，压缩预训练模型和直接训练小网络模型。本文提出了一类允许模型开发人员选择与其应用程序的资源限制（延迟，大小）相匹配的小型网络架构。MobileNets主要侧重于优化延迟，但也能够产生小型网络。很多关于小型网络的论文只关注大小，却不考虑速度问题。记录：需要注意的是目前在应用于移动和嵌入式设备的深度学习 网络研究主要有两个方向：1）直接设计较小且高效的网络，并训练。本文属于这方面，另外比较经典的如GoogleNet提出Inception思想，采用小卷积。Network in Network 改进传统CNN，使用1x1卷积，提出MLP CONV层Deep Fried Convents采用Adaptive Fast-food transform重新 参数化全连接层的向量矩阵。SqueezeNet设计目标主要是为了简化CNN的模型参数数量，主要采用了替换卷积核3x3为1x1，使用了deep compression技术对网络进行了压缩 。Flattened networks针对快速前馈执行设计的扁平神经网络。2）对预训练模型进行压缩小、分解或者压缩。采用hashing trick进行压缩；采用huffman编码；用大网络来教小网络；训练低精度乘法器；采用二进制输入二进制权值等等。 3.MobileNet 架构首先描述了使用深度可分解滤波（depth wise separable filters）建立的MobileNets核心层；然后介绍了MobileNet的网络结构，并且总结了两个模型收缩超参：宽度乘法器和分辨率乘法器（width multiplier和resolution multiplier）。3.1.深度可分解卷积（Depthwise Separable Convolution）MobileNet模型机遇深度可分解卷积，其可以将标准卷积分解成 一个深度卷积和一个1x1的点卷积。3.2. 网络结构和训练3.3.宽度乘法器（Width Multiplier）3.4. 分辨率乘法器（Resolution Multiplier） 此部分待更新 4.实验 本节首先调查了深度卷积（ depthwise convolution）的影响，以及通过减小网络宽度 而不是减少层数来选择压缩的 模型。然后基于两个超参：宽度乘法器和分辨率乘法器（width multiplier和resolution multiplier）进行网络收缩，并把其和现阶段主流的模型进行比较。研究结果表明MobileNet可以应用于许多不同的任务。4.1模型选择 首先比较了深度可分解卷积的MobileNet和全卷积的模型，如表4所示 ，使用深度可分解 卷积和全卷积相比，在ImageNet的精确度只下降了1%，但是Mult-Adds和参数大大节省。 表5显示，在计算和参数数量相似时，更浅的模型比更小的模型精度低3%。 4.2.模型超参收缩 表6显示宽度乘法器（width multiplier）超参α减小时，模型准确率随模型的变小而下降。 表7 显示分辨率乘法器（resolution multiplier）超参ρ 减小时，模型准确率随模型的分辨率变小而降低。 表8中将MobileNet和GoogleNet、VGG16进行了比较。表9比较了两个超参的变化4.3.细粒度识别任务本文在Stanford Dogs数据集上训练了MobileNet以应对细粒度识别任务。本文扩展了指定对比方法［19］并且收集了一个更大且包含更多噪声的数据集超过了指定方法［18］的规模。采用了网络噪声数据去预训练细粒度分类狗的 识别模型，然后在Stanford Dogs训练集上进行微调。结果如表10。MobileNet能够在极大的较少计算和尺寸的情况下获得接近于［18］的结果。4.4.大规模地理定位PlaNet的任务是用于确定在一张照片在哪个地理位置进行拍摄的分类问题。该方法将地球划分进一个个网格单元集合到目标类别，用数百万计的有地理位置标记的图片训练卷积 神经网络。PlaNet已经能够成功对各种各样的照片进行地理位置标记，并且处理相同任务性能超过了Im2GPS。在相同的数据上采用MobileNet架构重新训练PlaNet，其结果如表11所示，MobileNet版本和PlaNet相比，规模小了 很多，性能只降低了很少，据此来说，其仍然超过了Im2GPS4.5.人脸属性提取 MobileNet的另一个应用场景是压缩具有未知且复杂训练程序的大型系统。在人脸属性分类任务中，我们证明了MobileNet和distillation间的协同关系，一种针对深度网络的知识迁移技术。我们试图去简化一个具有7500万参数和16亿Mult-Adds大型人脸属性分类器。该分类器在一个类似于YFCC100M的多属性数据集上进行训练。采用MobileNet架构去提取一个人脸属性分类器。distillation是通过训练分类器模型一个更大的模型输出，而不是采用人工标注，因而能够从大型（无限大潜在可能）未标注数据集训练。结合distillation的可扩展性和MobileNet的简洁参数化，终端系统不仅不要求正则化，而去反而表现出更好的性能。如表12。4.6.目标检测MobileNet也能够作为一个高效的基网络被部署到现代目标检测系统。基于最近2016 COCO挑战赛的获胜者的工作，我们应对目标检测任务在COCO数据集上训练了MobileNet，并进行了比较。表13列出了在Faster RCNN和SDD框架下，MobileNet、VGG以及Inception V2的比较。实验中，SSD以300的输入分辨率于分别是300和600输入分辨率的 Faster RCNN进行比较，在两个框架下，Mobile Net性能不低于其它两个网络结果，且计算复杂性和模型都相对更小。 记录：此处感觉采用VOT Challenge 的数据集能够更有效客观的进行目标检测的结果评测，不知道为什么采用了COCO。 4.7.Face EmbeddingsFaceNet是目前state-of-art的人脸识别模型，它基于triplet loss建立face Embedding。为了建立移动FaceNet模型，我们采用distillation通过最小化FaceNet和MobileNet在训练数据上的平方差（squared differences ）来训练。表15可以看到非常小的MobileNet模型结果。 5.总结本文提出了一个基于深度可分解卷积（depthwise separable convolutions）的新模型架构MobileNets。分析了 决定高效模型的重要设计思路。然后，本文讲解了如何使用宽度乘法器（width multiplier）和分辨率乘法器（resolution multiplier），通过权衡较为可靠的精确度来减小尺寸大小和延迟时间，来构建更小更快的MobileNets。将不同的MobileNets和主流的模型进行比较，展现了 MobileNets在大小、速度和精确度这些特性都具有明显优势。最后，我们通过一系列任务的应用，验证了MobileNets的广泛适用性。下一步，本文的计划是在TensorFlow发布他们的模型。 记录：MobileNet应该是目前在设计小网络方向性能比较比较比较好的论文，论文中并看不出其具体的fps的数据，也是论文的缺憾，不过预计应该会有不错的性能。其也是深度学习网络应用到嵌入式和移动设备的一个比较好的参照。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>嵌入式神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云服务器搭建SVN版本控制工具]]></title>
    <url>%2F2017%2F12%2F23%2F%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BASVN%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[阿里云服务器搭建SVN版本控制工具记录在终端下采用ssh root@47.92.3.188连接我的服务器进入远程主机操作修改阿里云主机名称命令：hostnamectl set-hostname 新主机名 SVN部署操作检查是否安装地板本SVN：rpm -qa subversion卸载旧版本svn:yum remove subversion安装svn服务器端：yum install subversion执行以下命令：cd /usr/local 进入目录mkdir svnRepo 创建SVN目录chmod -R 777 svnRepo 修改权限777svnadmin create /usr/local/svnRepo/test_server 创建一个svn版本仓库test_server(test_server可以随便取名字） cd svnRepo/test_server/conf 1）修改该目录下三个配置文件vi svnserve.conf把 anon-access = readauth-access = writepassword-db= passwdrealm = test_server前#号和空格去掉，变成anon-access = none //修改成noneauth-access = writepassword-db= passwdrealm = test_server //改成自己的版本库保存退出2）vi authz //文件，创建svn组和组用户的权限test_server //创建test_server组，并制定三个用户whl，wxr，ryg[/] //制定根目录下的权限[test_server:/]//制定版本分支目录下的权限@test_server = rw // test_server组用户权限为读写× = r //其他用户只有读权限保存退出3）修改或创建用户密码vi passwd[users]whl = Jitu2017wxr = Jitu2017ryg = Jitu2017保存退出 设置自启动vi /etc/rc.local //打开自启动文件 文件内容如下 !/bin/bashTHIS FILE IS ADDED FOR COMPATIBILITY PURPOSES# It is highly advisable to create own systemd services or udev rulesto run scripts during boot instead of using this file.# In contrast to previous versions due to parallel execution during bootthis script will NOT be run after all other services.# Please note that you must run ‘chmod +x /etc/rc.d/rc.local’ to ensurethat this script will be executed during boot.touch /var/lock/subsys/local 添加下面一行svnserve -d -r /usr/local/svnRepo/avalon_server保存退出ps aux | grep ‘svn’ 查找所有svn启动的进程杀死，然后启动svn svnserve -d -r /usr/local/svnRepo/first 启动svn(可以把这个放到/etc/local/rc.local文件中，实现开机自启动)sudo netstat -anp | grep svnserve //验证是否开启成功 关闭svnservesudo pstree | grep svn #查看 sudo killall svnserve #关闭 SVN版本库起动方式: 1：单版本库起动 svnserve -d -r /usr/local/svnRepo/test_server2：多版本库起动 svnserve -d -r /usr/local/svnRepo区别在于起动svn时候的命令中的启动参数-r指定的目录。 连接 svn://47.92.3.188:3690命令行下采用svn checkout svn://47.92.3.188:3690 已经过iOS PHP等项目版本控制验证]]></content>
      <categories>
        <category>版本控制</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PVA Faster RCNN配置安装及使用]]></title>
    <url>%2F2017%2F12%2F22%2FPVA-Faster-RCNN%E9%85%8D%E7%BD%AE%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[论文：PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection简介PVANET是目前目标检测算法比较好的实现，也是Region Proposal系列方法的一个新实现，目前达到了real-time的单张图像40ms，且在VOC12上达到了精度83.8%，是region proposal目前精度最高的，也突破了region proposal系列方法RCNN、Fast RCNN、Faster RCNN的精度高不能实时的问题。目前回归类方法最新的SSD以及YOLO9000虽然速度很快，但是精度比其差了不少。所以PVANET还是值得实现的。论文地址：https://www.arxiv.org/pdf/1608.08021v3.pdfgithub项目:https://github.com/sanghoon/pva-faster-rcnn实现环境操作系统：Ubuntu 16.04LTS显卡：NVIDIA GTX 1080TICUDA8.0Cudnn V5.1其它配置忽略（不重要） Caffe General Dependencies安装 sudo apt-get update sudo apt-get upgrade sudo apt-get install -y build-essential cmake git pkg-config sudo apt-get install -y libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install -y libatlas-base-dev sudo apt-get install -y --no-install-recommends libboost-all-dev sudo apt-get install -y libgflags-dev libgoogle-glog-dev liblmdb-dev (Python general) sudo apt-get install -y python-pip \#(Python 2.7 development files) sudo apt-get install -y python-dev sudo apt-get install -y python-numpy python-scipy OpenCV安装此处根据git master branch安装的3.2.0-dev版本 \#Build tools: sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev cd ~ git clone https://github.com/opencv/opencv.git cd opencv mkdir build cd build cmake -D CMAKE_BUILD_TYPE=RELEASE \ -D CMAKE_INSTALL_PREFIX=/usr/local \ -D WITH_CUDA=ON \ -D WITH_CUBLAS=1 \ -D INSTALL_PYTHON_EXAMPLES=ON \ -D BUILD_EXAMPLES=ON .. make all -j16（此处根据你自己的计算机性能进行安装） sudo make -j16 install sudo ldconfig 检查安装 $ python \>>>import cv2 \>>>cv2.__version__ 输出：'3.2.0-dev' 有关CUDA 8.0+Cudnn5.1的安装请自行百度 pva-faster-rcnn的搭建 1.获取项目 git clone --recursive https://github.com/sanghoon/pva-faster-rcnn.git 2.编译建立Cython模块 安装python依赖 sudo pip install Cython sudo pip install easydict cd pva-faster-rcnn/lib 此处需要修改lib下的setup.py第135行 GPU 计算能力查看地址'nvcc': ['-arch=sm_35', https://developer.nvidia.com/cuda-gpus ![](http://upload-images.jianshu.io/upload_images/3478042-7250cd061ba5e39a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 比如我的是1080TI，计算能力为6.1，则修改为 'nvcc': ['-arch=sm_61', 执行 make all -j16 进行编译 3.编译建立Caffe和pycaffe cd pva-faster-rcnn/caffe-fast-rcnn cp Makefile.config.example Makefile.config gedit Makefile.config 修改Makefile.config文件: 需要修改的行如下 USE_CUDNN := 1 OPENCV_VERSION := 3 #CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda CUDA_DIR := /usr/local/cuda-8.0 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-Linux-gnu/hdf5/serial/ INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-Linux-gnu/hdf5/serial/ #Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 #Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) USE_PKG_CONFIG := 1 Makefile修改 cd ~/pva-faster-rcnn/caffe-faster-rcnn gedit Makefile 需要注意的是暂时不要采用最新的cudnn v6版本，我使用的时候发觉会出错，v5.1版本修改此配置。 原有 NVCCFLAGS += -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) 修改为 NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) 更新caffe-fast-rcnn的caffe部分，因为此项目caffe不是最新涉及到cudnn的计算会报错。 # Make sure to clone with --recursive cd ~/pva-faster-rcnn/caffe-fast-rcnn git remote add -f caffe https://github.com/BVLC/caffe.git git merge -X theirs caffe/master gedit include/caffe/layers/python_layer.hpp \# Remove self_.attr("phase") = static_cast(this->phase_); cd ~/pva-faster-rcnn/caffe-faster-rcnn/python for req in $(cat requirements.txt); do sudo -H pip install $req --upgrade; done 编译安装 mkdir build cd build cmake .. make -j16 all make -j16 pycaffe make -j16 install Caffe路径设置 gedit ~/.bashrc export CAFFE_ROOT=~/py-faster-rcnn/caffe-fast-rcnn export PYTHONPATH=~/py-faster-rcnn/caffe-fast-rcnn/python:$PYTHONPATH source ~/.bashrc 检查安装 $ python >>> import caffe >>> caffe.__version__ sudo -H pip install easydict sudo apt-get install python-gi-cairo python-tk 4.下载预训练模型如果有VPN网速好的话直接采用pva-faster-rcnn/models下的的shell脚本进行下载此处采用百度网盘下载链接：http://pan.baidu.com/s/1kVRRPDd 密码：1cdt1、打开文件将test.model放入$pva-faster-rcnn/models/pvanet/full/这个目录下2、将test(1).model重命名为test.model放入$pva-faster-rcnn/models/pvanet/comp/目录下5.voc2007数据集下载打开终端（任何目录）输入： wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 解压（严格按照此顺序）： tar xvf VOCtrainval_06-Nov-2007.tar tar xvf VOCtest_06-Nov-2007.tar tar xvf VOCdevkit_08-Jun-2007.tar 将生成的VOCdevkit文件夹更名为VOCdevkit2007移动至$pva-faster-rcnn/data/目录下面6.测试需要注意的是缺少test.pt，请将pvanet_obsolete下的对应文件复制过来即可终端输入： cd $pva-faster-rcnn 1、full/test.model测试： ./tools/test_net.py --gpu 0 --def models/pvanet/full/test.pt --net models/pvanet/full/test.model --cfg models/pvanet/cfgs/submit_0716.yml 2、Comp/test.model测试： ./tools/test_net.py --gpu 0 --def models/pvanet/comp/test.pt --net models/pvanet/comp/test.model --cfg models/pvanet/cfgs/submit_0716.yml 此测试会得到系列类别的AP值3.模型的可视化检测demo此处需要重新编写demo.py文件可在我github下fork的分支获取该文件demo.py文件https://github.com/YgRen/pva-faster-rcnn cd pva-faster-rcnn 执行 ./tools/demo.py --gpu 0 --def models/pvanet/comp/test.pt --net models/pvanet/comp/test.model 生成可视化结果 记录：关于自定义数据集的训练和调参后期会进行发布，关于VOC07和12数据集的训练，直接参照github项目地址操作即可，最近会更新论文精读。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
