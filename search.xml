<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PVANET论文阅读笔记]]></title>
    <url>%2F2018%2F01%2F07%2FPVANET%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文地址：https://arxiv.org/pdf/1608.08021.pdf项目地址：https://github.com/YgRen/TFFRCNN注：本文是我在2017年6月读的一篇文章，这篇文章最初开源的代码是这个地址: https://github.com/CharlesShang/TFFRCNN ，估计不是原作者，后来在测试时，发觉有问题我直接fork了一下，重写了demo测试页面，然后这个项目的PVANet的Train和Test部分网络结果都存在问题，我在本地进行了重写，如果有需要的话我近期会传到Github上去。具体的使用实现可参见PVANET的配置安装及使用 本文是发表于 NIPS 2016上的一篇文章，PVANET是韩国研究者（Intel Imaging and Camera Technology）结合了最新的技术，在TITAN X上实现了轻量级模型的实施目标检测任务，在PASCAL VOC上取得了第二名的成绩。 PVAnet是RCNN系列目标方向，基于Faster-RCNN进行改进，Faster-RCNN的基础网络可以使用ZF、VGG、Resnet等，但精度与速度难以同时提高。PVAnet的含义应该是Performance Vs Accuracy，意为加速模型性能，同时不丢失精度的含义。 摘要本文提出的方法通过调整和结合最近的技术创新来最小化计算代价（cost）在多类目标检测任务中取得了state-of-the-art的准确率。采用“CNN Feature Extraction + Region Proposal +ROI Classification”的结构，主要对特征提取（feature Extraction）部分进行重新设计。主要是因为Region Proposal部分计算量并不太大，而Classification部分可以常见的技术（如 truncated SVD）进行有效压缩。本文的设计原则是采用“less channels with more layers” 和采用一些building blocks，包括 concatenated ReLU、Inception、HyperNet。设计的网络是deep且thin的，在BN（batch normalization）和残差连接（residual connections）和基于plateau detection的laerning rate的调整的帮助下进行训练。在VOC2007上获得83.8%的mAP，在VOC2012上获得82.5%的mAP（第二名），而每张图片在单核Intel I7-6700K CPU上耗时750ms，在NVIDIA Titan X GPU上每张图只需要46ms。理论上，本文采用的方法，比较于VOC2012比赛的冠军ResNet-101之需要花费12.3%的计算代价。 1.引言近几年CNNs在目标检测领域获得了巨大的提升。由于一些突破性的工作，现在的目标检测系统已经满足了自动驾驶和监控等商业领域的精度要求。然而，就检测速度而言，即使是最好的算法仍然需要较大的计算代价（heavy computational cost）。近来网络压缩和量化技术在网络设计层减少计算代价比较重要。本文提出了针对目标检测任务名为PVANET的轻量级特征提取网络结果，能够在比较其他state-of-the-art方法上不丢失准确率的前提下获得实时的目标检测性能。（1）计算代价（Computational Cost）：输入1065*640大小的图像在特征提取时需要7.9GMAC（ResNet-101:80.5GMAC）（2）运行时性能（Runtime performance）：750ms/image (1.3FPS) 在 Intel i7-6700K CPU (单核)；46ms/image (21.7FPS) on NVIDIA Titan X GPU（3）准确率（Accuracy）: 83.8% mAP on VOC-2007; 82.5% mAP on VOC-2012 (2nd place)设计的关键原则是“less channels with more layers”（即更多的网络层数，更少的通道数，也就是通过减少特征维度，来减少计算量），同时，本文的网络结构采用了一些最近提出的网络块结构（building blocks），其中的一些在目标检测任务中的性能还没有得到证明：（1）Concatenated rectified linear unit (C.ReLU)[2] 被用在我们的CNNs的初期阶段来减少一半的计算数量而不损失精度。（2）Inception[3] 被用在剩下的生成feature的子网络中。一个Inception模块产生不同大小的感受野（receptive fields）的输出激活值，所以增加前一层感受野大小的变化。我们观察叠加的Inception 模块可以比线性链式的CNNs更有效的捕捉大范围的大小变化的目标。（3）采用multi-scale representation的思想（像HyperNet[4] 中）结合多个中间的输出，所以，这使得我们可以同时考虑多个level的细节和非线性。本文展示了thin且deep的网络能够在BN（batch normalization）[5]和残差连接（residual connections）[1]和基于plateau detection[1]的laerning rate的调整的帮助下非常有效地进行训练。本文剩下的部分，在第二部分简洁的对网络结构设计机械能介绍，并在第三部分对PVANET的结构细节进行了总结，最后在第四部分详细的描述了训练和测试过程，提供了在VOC2007和VOC2012伤的实验结果。 2.网络设计细节2.1C.ReLU: Earlier building blocks in feature generationC.ReLU来源于CNNs中间激活模式引发的。在初期阶段，输出节点倾向于是”配对的”，一个节点激活是另一个节点的相反面。根据这个观察，C.ReLU减少一半输出通道(output channels)的数量，通过简单的连接相同的输出和negation使其变成双倍，即达到原来输出的数量，这使得2倍的速度提升而没有损失精度。C.ReLU的实现细节如下。比较于原始的C.ReLU，本文在concatenation（串联）之后，增加了scaling and shifting（缩放和移动），这样的操作允许每个channel（通道）的斜率和激活阈值与其相对的channel不同 2.2 Inception: Remaining building blocks in feature generation对于目标检测任务来说Inception没有广泛的使用在现存的网络中，也没有验证其有效性。我们发现Inception是用于捕获输入图像中小目标和大目标的最具有cost-effective（成本效益）的building blocks之一。为了学习捕获大目标的视觉模式，CNNs的输出特征应该对应于足够大的感受野，这可以很容易的通过叠加 33或者更大的核(kernel)卷积实现。在另外一方面，为了捕获小尺寸的物体，输出特征应该对应于足够小的感受野来精确定位小的感兴趣区域。图二所示的Inception可满足以上需求。最后面的11的conv通过保留上一层的感受野（receptive field），扮演了关键的角色。仅仅通过增加输入模式的非线性，它减慢了一些输出特征的感受野的增长，使得可以精确地捕获小尺寸的目标。图三展示了Inception的实现细节，采用了2个顺序的3x3卷积来取代5x5的卷积。 2.3 HyperNet: Concatenation of multi-scale intermediate outputs （连接多尺度的中间输出，也集考虑到层次的特征）多尺度表示和它们的结合在最近的许多深度学习任务中[4] [6] [7] 被证明是有效的。细粒度细节与高度概括的语义信息的结合有助于随后的region proposal网络和分类网络检测不同尺度的目标。然而，因为直接连接所有的abstraction layers也许产生有很多计算需要求的冗余信息（redundant information），我们需要仔细设计不同 abstraction layers和layers的数量。如果选择的层对于object proposal和分类太早的话，当我们同时考虑增加计算复杂度的话帮助很小。本文的设计选择不同于ION[6]和HyperNet[4]，它结合了1）最后一层和2）两个中间层，它们的尺寸分别是最后一层的2倍和4倍。本文采用了中等大小的层作为参考尺度（= 2x），并且分别concatenate（串联）4x尺寸的层和采用缩小（pooling）的层和采用放大（linear interpolation，线性插值）的层。 2.4深度网络训练普遍认为，随着网络的越来越深，网络的训练变得越来越困难。本文同采用残差结构[1]来解决这个问题。与原始的残差训练方法不同的是，我们将残差连接到Inception layers以稳定深度网络框架的后半部分。在所以的ReLU激活层之前采用BN[5]，mini-batch样本统计用于pre-training阶段，并且随后moving-averaged统计作为固定的尺度和移动的参数。学习率策略对网络成功训练也很重要。本文的策略是：基于plateau detection动态地控制学习率。我们采用移动平均数(moving average)损失并且如果在某次的迭代周期期间，其改进低于一个阀值，则将其确定为on-plateau。无论何时plateau被检测到，学习率减少一个常数因子。在实验中，我们的学习率策略对准确率有一个显著的结果。 3 Faster R-CNN with our feature extraction network表一展示了PVANET的整个网络结构。在初期阶段（conv1_1,…,conv3_4），C.ReLU用在卷积层来减少一半KK conv的计算消耗。11conv layers添加在KK conv的前面和后面，目的是减少输入的大小然后分别表示的能力。三个中间输出conv3_4(with down-scaling)、conv4_4和conv5_4(with up-scaling)结合到512-channel多尺度输出特征（convf），之后被送到Faster R-CNN模型： （1）为了提高计算效率。仅convf的前128通道被送到region proposal network（RPN）。我们的RPN是一系列”33conv(384channels- 1x1 conv(25x(2+4) = 150 channels)”层来生成regions of interest（RoIs）。 （2）R-CNN采用convf中的全部512 channels。对于每一个RoI，RoI pooling 生成 66512 tensor（张量），之后通过一系列的”4096-4096-(21+84)”的输出节点的全连接层。 4.实验结果4.1.训练和测试 PVANET 在1000类别的 ILSVRC2012 训练图像上进行预训练。所有的图像被调整到256256大小，之后随机裁剪成192192的patches被作为网络的输入。学习率的初值设为0.1，之后只要检测到plateau时就以因子减少。若学习率小于1e-4就终止预训练，这个过程大约需要2M次迭代。 之后，PVANET在联合的数据集（MS COCO trainval，VOC2007 trainval 和 VOC2012 trainval）训练。之后用VOC2007 trainval 和 VOC2012 trainval进行fine-tuning（微调）也是需要的，因为在MS COCO和VOC竞赛的类别定义是稍有不同。训练数据被随机的调整大小，图像较短的边的长度在416和864之间。 对于PASCAL VOC评价，每个输入图像调整大小使其短的一边为640。所有有关Faster R-CNN的参数被设置在原始的work中，除了在极大值抑制（NMS：non-maximum suppression）（=12000）前的proposal boxes的数量和NMS的阈值（=0.4）。所有的评估在单核的Intel i7-6700K CPU和 NVIDIA Titan X GPU 上完成。 4.2.VOC2007Table2展示了我们的模型在不同配置下的准确率。由于Inception和多尺度feature，我们的RPN产生的initial proposals非常的准确。因为结果表示超过200个proposals没有对检测的准确率起到显著的帮助，我们在剩下的实验中把proposals 的数量固定在200个。我们也测量了当迭代回归不使用时用带有bounding-box voting的性能。 Faster R-CNN包含的FC layer，它容易压缩并且准确率没有显著的下降。我们通过the truncated singular value decomposition (SVD)把”4096-4096”的FC layer压缩成”512-4096-412-4096”，之后进行一些微调。这个压缩的网络达到了82.9%mAP（-0.9%）而且运行达到了31.3FPS（+9.6FPS） 4.3.VOC2012Table3总结了PVANET+和一些先进的网络在PASCAL VOC2012排行榜上的比较。我们PVANET+达到了82.5%mAP，位列排行榜的第二名，超过其他竞争者除了”Faster R-CNN + ResNet-101”。然而第一名使用的ResNet-101比PVANET大很多同时一些耗时的技术，例如global contexts（全局上下文）和多尺度测试，导致至少比我们慢40倍。在表3,我们也比较computation cost。在所有超过80%mAP的网络中，PVANET是唯一一个运行时间小于50ms的。考虑精度和计算cost，PVANET+是排行榜中最有效的网络。 5.总结本文展示了当前的网络是高度冗余的，并且设计了一个轻量级的网络能够有足够能力应对复杂的视觉任务。精心的采纳和结合最近的深度学习中的创新成功是我们能够重新设计Faster RCNN框架的特征提取部分从而最大化计算效率。即使我们提出的网络是为目标检测任务设计的，当我们相信本文的设计原则能够广泛应用于诸如人脸识别、语音分析的任务中。 评注这篇论文发表于NIPS 2016上的一篇关于目标检测的论文，在这篇论文之前效果最好的是Faster RCNN++和R-FCN，二者各有利弊，Fast RCNN ++在精度上有提升，但是速度还是比较慢，R-FCN解决了速度问题，精度却稍逊一筹。目标检测的Baseline基本沿袭了这一思路： CNN Featue Extract + Region Rroposal + RoI Classification。本文作者从CNN Feature Extract入手，改进了特征提取网络结果，最终达到了非常好的效果。从文章内容来上说本文的主要工作就是在思路上对CNNN Feature Extract这一角度的优化处理，从而取得了较好的性能，从理论上来说，只是对现有的C.ReLU、Inception、HyperNet的多层特征融合的思路的一个整合，借鉴了比较有突破性的结果，结合BN和各种学习调参策略。从工程的角度来说，这是一篇不错的文章，在工程上取得了较好的性能，纯粹从学术角度上，亮点并不多。同时，该网络训练似乎存在训练不佳的问题。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster RCNN论文阅读笔记]]></title>
    <url>%2F2018%2F01%2F03%2FFaster-RCNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文题目：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks论文地址：https://arxiv.org/pdf/1506.01497.pdf项目地址：Matlab版本、Python版本 本文发表于NIPS 2015，是继RCNN、Fast RCNN之后，目标检测方向代表人物Ross Girshick团队在2015年的有一力作。简单网络目标检测速度达到17fps，在PSCAL VOC上的准确率为59.9%；复杂网络达到5fps，准确率78.8%。 Faster RCNN其实可以分为4个主要内容： （1）Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。（2）Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。（3）Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。（4）Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 摘要目前顶级的目标检测算法依赖于Region Propsal算法来假设目标定位，SPPNet和Fast RCNN已经减少了检测网络的运行时间，Region Proposal计算成为目前算法的瓶颈。本文的工作是引入了一个Region Proposal Network（RPN）和检测网络共享整个图像的卷积特征，使得Region Proposals操作近乎cost-free。RPN是一个全卷积网络，可以同时预测每一个位置的目标边界和目标评分。RPN采用端到端的训练来生成高质量的Region Proposals，然后采用Fast RCNN来检测。通过最近神经网络受欢迎的术语 attention机制，共享卷积特征，进一步的将RPN和Fast RCNN合并成一个单一网络，RPN组建告诉我们整个网络往哪里看。针对VGG16深度模型，本文的检测系统在PASCAL VOC 2007 、2012和MS COCO 数据集上每张图片有300个prposals的时候在GPU上运行有5fps的速度。在ILSVRC和COCO 2015的比赛中，Faster RCNN和RPN在多个任务中获得第一。 评注：从RCNN到Fast RCNN，再到本文的Faster RCNN，目标检测的四个基本步骤（候选区域的生成、特征提取、分类、位置精细调整）被统一到了一个深度网络框架之内。没有重复计算，完全在GPU中完成，大大提高了运行速度。Faster RCNN可以简单看作“Region Proposal Networks + Fast RCNN”的系统，采用RPN代替Fast RCNN的Selective Search方法。本文着重解决了此系统的三个问题：(1).如何设计RPN(2).如何训练RPN(3).如何让RPN和Fast RCNN网络共享权值提取网络 1.引言区域推荐方法(比如[4])和基于区域的卷及神经网络（RCNNs）[5]的成功推动了物体检测水平的进步。尽管RCNNs刚开发出来时[5]十分费时，经过[1][2]的跨推荐区域的共享卷积的改进，已经大幅消减了开销。近期大作Fast R-CNN[2]，如果不考虑区域推荐的耗时，使用超深度网络[3]已经达到几乎实时的处理速度。但推荐显然是最先进检测系统的瓶颈。区域推荐算法主要依赖简单的特征和经济的推理机制。最受欢迎的方法——选择性搜索[4]是基于低层次的人工特征贪婪地进行超级像素合并。而跟有效的检测网络[2]相比，选择性搜索的就慢了一个数量级，CPU上每张图片耗时2秒。EdgeBoxes[6]当前做到了速度和推荐质量的最佳平衡。然而，在整个检测网络中，区域推荐这一步仍然是主要耗时阶段。你也许会注意到快速的基于推荐的CNNs充分利用了GPU，而区域推荐算法都是CPU中实现的。所以进行这个时间比较是不公平的。如果想加速它，用GPU实现就好了呀。这也许是个有效的工程化解决方案，但重新实现仍然会忽略下游的检测网络，因而也失去了共享计算的好机会。本文将向您展示一个算法上的改变：使用深度卷积神经网络计算推荐区域，将引出一个优雅而高效的解决方案，在给定检测网络完成的计算的基础上，让区域的计算近乎为0。鉴于此，我们向大家隆重介绍这个新型的区域推荐网络（Region Proposal Networks，RPNs），它和当今世界最棒的检测网络[1][2]共享卷积层。通过在测试阶段共享卷积，让计算推荐区域的边际成本变得很低（比如每张图片10ms）。我们观察到像Fast R-CNN这样的基于区域的检测器锁使用的卷积特征图也可以用来生成推荐区域。在这些卷积层的特征之上，我们通过添加一些额外的卷积网络引入一个RPN，可以和回归约束框、物体打分相并列。RPN是一种完全卷积网络(FCN)[7]，可以为特定任务进行端到端的训练来产生检测推荐。RPNs被设计用来高效地预测各种尺度和宽高比的区域推荐。对称之前的[8][9][1][2]，他们均使用图像金字塔（图1，a）或特征的金字塔（图1，b），我们则使用“锚点”盒（“anchor” boxes）作为不同尺度和宽高比的参照物。我们的模式可以看做是一个回归参照物的金字塔（图1，c），这避免了穷举各种尺度和宽高比的图像或过滤器。这个模型在单一尺度图像的训练和测试时表现优异，因而运行速度大为受益。为了统一RPNs和Fast R-CNN[2]物体检测网络，我们提出一种介于区域推荐任务调优和之后的物体检测调优之间的训练方法，同时还能保证固定的推荐。这个方法可以很快收敛，并产生一个统一的网络，该网络在两个任务上共享卷积特征。我们在PASCAL VOC检测benchmarks[11]上全面评估了我们的方法，RPNs结合Fast R-CNNs可以比选择性搜索结合Fast R-CNN有更高的准确度。于此同时我们的方法摒弃了选择性搜索在测试阶段几乎所有的计算负担，有效推荐的运行时间只有区区的10毫秒。使用十分耗时的超深度模型[3]，我们的检测方法仍然可以在GPU上达到5fps的速度，这使得物体检测系统在速度和精度上都变得更加使用。我们也报告了在MS COCO数据集[12]上的结果，探究了PASCAL VOS上使用COCO数据集带来的提升。代码现在开放在 https://github.com/shaoqingren/faster_rcnn (in MATLAB)和https://github.com/rbgirshick/py-faster-rcnn (in Python)。本文的一个早期版本发布在[10]上。从那时起，RPN和Faster R-CNN的框架就已经被采用，并应用到其他的方法中，比如3D物体检测[13]，基于组件的检测[14]，实力分割[[13]和图像字幕[16]。我们的快速而有效的物体检测系统已经构建在想Pinterests[17]这样的商业系统中，提升了用户交互。在ILSVRC和COCO 2015竞赛中，Faster R-CNN和RPN是多项分赛长的第一名[18]，包括ImageNet 检测，ImageNet定位，COCO检测和COCO分割。RPNs从数据中完全学会了推荐区域，而且使用更深或更有表达力的特征（比如101层的Resnet[18]）效果会更好。Faster R-CNN和RPN也用于多个其他领先名词的团队所使用。这些结果都说明我们的方法不仅实用省时，而且有效精准。 2.相关工作物体推荐。有大量的推荐方法。有一些综述和这些方法的比较可见于[19], [20], [21]。广泛使用的方法很多基于grouping super-pixels (比如,Selective Search [4], CPMC [22], MCG [23])，还有一些基于滑动窗口(e.g., 比如窗口的物体属性objectness in windows[24], EdgeBoxes [6])。物体推荐方法也经常独立于它的检测器而被很多外部的模块使用 (比如, Selective Search [4] object detectors, RCNN [5], 和Fast R-CNN [2])。用于物体检测的深度网络。R-CNN方法[5]端到端地训练CNNs，用于将推荐区域分类成物体类别或背景。R-CNN主要扮演了分类器的角色，它并不预测物体的边框（除了用于约束框回归的净化模块）。他的精度依赖于区域推荐模块的性能（见[20]中的比较）。多篇论文推荐是用深度网络预测物体约束框 [25], [9], [26], [27]。OverFeat方法中，一个全连接网络用于训练预测定位任务的单一物体的框坐标。为了检测多个特定类的物体又将全连接层转变成卷积层。MultiBox方法[26][27]也使用网络产生推荐，它的最后一个全连接层可以同时预测多个未知类的框，推广了OverFeat的“单框”风格。这些未知类方框也被R-CNN[5]所使用。MuiltiBox推荐网络应用于单张图片的一个裁切，或者一个大型图片的多个裁切（比如224×224），和我们的全卷积模式完全不同。MultiBox并不在推荐和检测网络之间共享特征。后面结合我们的方法，我们将深入讨论OverFeat和MultiBox。和我们的工作同时进行的DeepMask方法[28]也被开发出来用于语义推荐。卷积计算的共享 [9], [1], [29],[7], [2]，已经越来越受关注。OverFeat[9]中针对分类、定位、检测时会只从一个图像金字塔计算卷积特征。尺寸自适应的SPP[1]也是建立在共享卷积特征图智商的，在基于区域的物体检测[1][30]和语义分割[29]上很有效。Fast R-CNN[2]使得端到端的检测器训练全部建立在共享卷积特征之上，表现出了有引人注目的精度和速度。 3.Faster RCNN我们的物体检测系统，成为Faster R-CNN有两个模块组成。第一个模块是深度卷积网络用于生成推荐区域，第二个模块是Fast R-CNN检测器[2]，使用推荐的区域。整个系统是一个单一的统一的网络（图2）。使用近期流行的属于“注意力”[31]机制，RPN模块告知Fast R-CNN看向哪里。3.1节我们介绍网络的设计和特性。3.2节，我们开发算法用于训练模块和特征共享。 3.1 区域推荐网络特征推荐网络接收任意尺寸的图像输入，输出一组矩形框代表物体推荐区域，每个区域都会有一个物体性的打分。我们使用完全卷积网络（FCN）[7]构建这个过程，本节将详细表述它。由于我们的终极目标是共享Fast R-CNN和物体检测网络[2]之间的计算力，我们假定两个网络可以共享一套卷积层。在实验中，我们研究了Zeiler和Fergus模型32，他们就共享了5个卷积层，还研究了Simonyan 和Zisserman模型[3] (VGG-16)，他们共享了13个卷积层。为了产生区域推荐，我们用一个小网络在最后一个卷积层的卷积特征图上滑动。每个滑动窗口都映射到一个更加低维度的特征（对ZF使用256，对VGG使用512，后面跟一个ReLU[33]）。这个特征再喂给两个并列的全连接层，一个框回归层（reg）和一个框分类层（cls）。本文中，我们使用n=3,一个在大图片（对于ZF和VGG来说，分别是171和228像素）十分有效的感受野大小。这个迷你网络在单一位置的示意如图3（左）。注意，由于迷你网络以滑动窗口的方式进行操作，全连接层是在全部空间位置共享的。这个架构很自然就就实现成一个nxn的卷积网络跟两个1×1的卷积网络层（分别是reg和cls）。3.1.1 锚点在每个滑窗位置，我们同时预测多个区域推荐，每个位置的最大滑窗推荐数量定位为k。这样reg层就有4k的输出编码k个框的坐标，cls就有2k的预测对象还是非对象的概率的打分。k个推荐是针对k个参考框进行参数化的，这个参考框我们称之为锚点。一个锚点就是正在关注的滑窗的中心，并和缩放比例、宽高比想关联（图3左）（译者注：就是滑窗中心坐标、缩放比例、宽高比形成的三元组决定一个锚点）。默认我们使用3个缩放尺度和3个宽高比，在每个滑动位置产生k=9个锚点。对于一个WxH（通常是2400）大小的卷积特征图，总共有WHk个锚点。在feature map上的每个特征点预测多个region proposals。具体作法是：把每个特征点映射回映射回原图的感受野的中心点当成一个基准点，然后围绕这个基准点选取k个不同scale、aspect ratio的anchor。论文中3个scale（三种面积\left{ 128^2, 256^2, 521^2 \right}），3个aspect ratio( {1:1,1:2,2:1} )平移不变性锚点我们方法有一个重要特性就是平移不变性。无论是锚点还是相对锚点计算推荐的函数都有这个特性。如果在一涨图片上移动一个物体，推荐也应该平移并且相同的函数应该能够在新的位置也计算出推荐来。我们的方法可以保证这种平移不变性。作为对比，MultiBox方法[27]使用k-means产生了800个锚点，却不能保持平移不变性。因此MultiBox不能保证在物体平移后产生同样的推荐。平移不变性可以缩减模型的大小。MultiBox有（4+1）x 800维的全链接输出层，而我们的方法只有（4+2）x9的卷积输出层，锚点数是k=9。结果，我们的输出层有2.8 x 10^4个参数（对于VGG-16而言是512 x （4 + 2）x 9），比MultiBox的输出层的6.1×10^6个参数（对GoogleNet[34]为1536x（4 + 1）x800）少了两个数量级。如果考虑特征映射层，我们的推荐层也还是少一个数量级。我们预期这个方法可以在PASCAL VOC这样的小数据集上有更小的过拟合风险。多尺度锚点作为回归参照物我们的锚点设计是解决多尺度问题的一种新颖形式。如图1所示，有两种流行的多尺度预测形式。第一种是基于图像/特征金字塔，也就是DPM[8]和基于CNN的方法[9][1][2]。图像被缩放到各种尺度，特征图(HOG[8]或深度卷积特征[9][1][2])也在每个尺度进行计算，参见图1（a）。这种方式通常很有用，但是很耗时。第二种方式是在特征图的多个尺度上使用滑窗。例如，在DPM[8]中，不同缩放比例的模型分开训练，使用了不同的过滤器尺寸（诸如5×7,7×5）。如果这种方式解决多尺度问题，可以看作是过滤器的金字塔，图1（b）。第二种方式通常和第一种方式联合使用[8]。作为比较，我们的基于锚点的方法是建立在锚点金字塔上的，是否高效。我们的方法使用不同尺度和不同宽高比的锚点作为参考分类和回归约束框。他之和单一尺度的图像和特征图有关，并且使用单一尺寸的过滤器，这些过滤器在特征图上进行滑动。我们通过实验显示了我们这个方法解决多尺度和多尺寸问题的效果（表8）。由于基于锚点的多尺度设计，我们可以和Fast R-CNN检测器[2]一样，只在单一尺度的图像上计算卷积特征。多尺度锚点的设计是不用额外计算开销共享特征解决多尺度问题的关键。3.1.2 损失函数为了训练RPNs，我们设计了针对每个锚点的二分类标签（是否是一个物体）。我们给两类锚点标记位正例：（i）和标注框最大重合的锚点 （ii）和任何标注框IoU重叠度超过0.7的。对于一个真实标注可能会产生多个正例锚点。通常第二类情况就足够确定正例了，但我们仍然采用第一类的原因是一些特别极端的案例里面没有正例。对于与标注框重叠度低于0.3的都标注为负例。既正且负的锚点对训练没有帮助。结合这些定义，我们参照Fast R-CNN中的多任务损失函数的定义我们的损失函数是： L( \{p_i\},\{t_i\})= \frac {1} {N_{cls}} \sum_i L_{cls}(p_i,p^*_i) + \lambda \frac {1} {N_{reg}} \sum_i p^*_iL_{reg}(t_i,t^*_i)这里，i是一个mini-batch中anchor的索引, $pi$是anchor i 为一个目标的预测概率，如果anchor为正，GT标签$p^_i$ 就是1,如果anchor为负，$p_i^$就是0。ti是一个向量，表示预测的包围盒的4个参数化坐标，ti* 是与正anchor对应的GT包围盒的坐标向量。分类损失$L{cls}$是两个类别（目标vs.非目标）的对数损失。对于回归损失，采用$L{reg}(t_i,t^_u)=R(t_i-t^_i)$ 其中R是[2]中定义的鲁棒的损失函数（smooth L1）。$P^*_iL{reg}$这一项意味着只有正anchor（$P^_i =1$ ）才有回归损失，其他情况就没有（$P^i=0$ ）。cls层和reg层的输出分别由{$p_i$}和{$t_i$}组成，这两项分别由$N{cls}$和$N{reg}$以及一个平衡权重λ归一化（早期实现及公开的代码中，λ=10，cls项的归一化值为mini-batch的大小，即$N{cls}=256$，reg项的归一化值为anchor位置的数量，即$N_{reg}$~2,400，这样cls和reg项差不多是等权重的。对于回归，我们学习[5]采用4个坐标：$t_x = (x-x_a)/w_a , ty = (y-y_a)/h_a$$t_w = log(w/w_a) , t_h = log(h/h_a)$$t_x^ = (x^- x_a)/w_a, t^_y=(y^-y_a)/h_a$$t^_w=log(w^/w_a), t^_h = log(h^/h_a)$x，y，w，h指的是包围盒中心的（x, y）坐标、宽、高。变量x，$x_a$，x*分别指预测的包围盒、anchor的包围盒、GT的包围盒（对y，w，h也是一样）的x坐标。可以理解为从anchor包围盒到附近的GT包围盒的包围盒回归。无论如何，我们用了一种与之前的基于特征映射的方法[1,2]不同的方法实现了包围盒算法。在[1, 2]中，包围盒回归在从任意大小的区域中pooling到的特征上执行，回归权重是所有不同大小的区域共享的。在我们的方法中，用于回归的特征在特征映射中具有相同的空间大小（3x3）。考虑到各种不同的大小，需要学习一系列k个包围盒回归量。每一个回归量对应于一个尺度和长宽比，k个回归量之间不共享权重。因此，即使特征具有固定的尺寸/尺度，预测各种尺寸的包围盒仍然是可能的。训练RPNRPN很自然地实现为全卷积网络，通过反向传播和随机梯度下降（SGD）[35]端到端训练。我们遵循[2]中的“image-centric”采样策略训练这个网络。每个mini-batch由包含了许多正负样本的单个图像组成。我们可以优化所有anchor的损失函数，但是这会偏向于负样本，因为它们是主要的。因此，我们随机地在一个图像中采样256个anchor，计算mini-batch的损失函数，其中采样的正负anchor的比例是1:1。如果一个图像中的正样本数小于128，我们就用负样本填补这个mini-batch。我们通过从零均值标准差为0.01的高斯分布中获取的权重来随机初始化所有新层（最后一个卷积层其后的层），所有其他层（即共享的卷积层）是通过对ImageNet分类[36]预训练的模型来初始化的，这也是标准惯例[5]。我们调整ZF网络的所有层，以及conv3_1，并为VGG网络做准备，以节约内存[2]。我们在PASCAL数据集上对于60k个mini-batch用的学习率为0.001，对于下一20k个mini-batch用的学习率是0.0001。动量是0.9，权重衰减为0.0005[37]。我们的实现使用了Caffe[38]。 3.2RPN和Fast RCNN共享特征迄今为止，我们已经描述了如何来为生成区域推荐训练网络，而不考虑基于区域的目标检测 CNN如何利用这些推荐框。对于检测网络，我们采用Fast R-CNN，接下来我们将介绍RPN和Fast RCNN共享卷积层的统一网络。RPN和Fast R-CNN都是独立训练的，要用不同方式修改它们的卷积层。因此我们需要开发一种允许两个网络间共享卷积层的技术，而不是分别学习两个网络。注意到这不是仅仅定义一个包含了RPN和Fast R-CNN的单独网络，然后用反向传播联合优化它那么简单。原因是Fast R-CNN训练依赖于固定的目标建议框，而且并不清楚当同时改变建议机制时，学习Fast R-CNN会不会收敛。虽然这种联合优化在未来工作中是个有意思的问题，我们开发了一种实用的3步训练算法，通过交替优化来学习共享的特征。（1）交替训练（Alternating training）。 在这个解决方案中，我们首先训练RPN，并使用这些推荐结果来训练Fast R-CNN。 由Fast R-CNN调整的网络然后被用于初始化RPN，并且这个过程被重复。 这是本文所有实验中采用的解决方案。（2）近似联合训练（Approximate joint training），在这个解决方案中，RPN和Fast RCNN网络在训练如图二的期间被合并。在每次SGD迭代过程中，前向传播生成的区域推荐就像固定的一样被对待，然后在训练Fast RCNN检测器的时候预计算推荐框。反向传播向往常一样对待，这里共享卷积层的反向传播信号来自PRN Loss和Fast RCNN Loss的被联合考虑。这个解决方案是容易实现的。但是这个方案忽略了倒数w.r.t这个推荐盒的左边也是网络的响应，因此是近似的。在我们的实验中，我们发现这个求解器产生了相当的结果，和交替训练相比，我们减少了大约20～50%的训练时间。这个解决方案可以在发布的Python代码中看到。（3）非近似联合训练（Non-approximate joint training），如上所述，RPN预测的边界框也是函数的输入。Fast RCNN的 ROI Pooling Layer接收卷积特征和预测的边界框作为输入，所以咋i 理论上有效的反向传播应该包括梯度w.r.t的盒坐标。这些梯度在上面的近似联合训练中被忽略。在非近似联合训练解决方案汇总，我们需要一个ROI Pooling Layer不同于w.r.t 盒坐标。这是个非常好的问题，可以通过ROI Wraping Layer的发展给出，这超出了本文讨论的范围。四步交替训练（4-Step Alternating Training），在本文中，我们采用了实用的四步训练算法来通过交替优化学习共享特征。在第一步中，我们按照3.1.3的描述训练RPN。这个网络用ImgeNet的预训练模型进行初始化，然后端到端的完成Region Proposal任务的微调。在第二步，我们采用Fast RCNN通过采用第一步RPN生成的Proposals单独训练了一个检测网络。这个检测网络也是通过ImageNet预训练模型进行初始化的。此时这个两个网络不共享卷积层。在第三部中，我们使用检测网络来初始化RPN的训练，但是我们固定共享的卷积层，只微调RPN单独的层。然后，这两个网络共享了卷积层。最后，保持卷积层共享的固定，我们微调了Fast RCNN的单独的层。就这样，两个网络共享了相同的卷积层并形成统一的网络。类似的交替训练可以更好的运行在更多的迭代次数上，但是观察到的提升可以忽略。 3.3实现细节我们训练、测试区域建议和目标检测网络都是在单一尺度的图像上[1, 2]。我们缩放图像，让它们的短边s=600像素[2]。多尺度特征提取可能提高准确率但是不利于速度与准确率之间的权衡[2]。我们也注意到ZF和VGG网络，对缩放后的图像在最后一个卷积层的总步长为16像素，这样相当于一个典型的PASCAL图像（~500x375）上大约10个像素（600/16=375/10）。即使是这样大的步长也取得了好结果，尽管若步长小点准确率可能得到进一步提高。对于anchor，我们用3个简单的尺度，包围盒面积为128x128，256x256，512x512，和3个简单的长宽比，1:1，1:2，2:1。注意到，在预测大建议框时，我们的算法考虑了使用大于基本感受野的anchor包围盒。这些预测不是不可能——只要看得见目标的中间部分，还是能大致推断出这个目标的范围。通过这个设计，我们的解决方案不需要多尺度特征或者多尺度滑动窗口来预测大的区域，节省了相当多的运行时间。图1（右）显示了我们的算法处理多种尺度和长宽比的能力。下表是用ZF网络对每个anchor学到的平均建议框大小（s=600）。跨越图像边界的anchor包围盒要小心处理。在训练中，我们忽略所有跨越图像边界的anchor，这样它们不会对损失有影响。对于一个典型的1000x600的图像，差不多总共有20k（~60x40x9）anchor。忽略了跨越边界的anchor以后，每个图像只剩下6k个anchor需要训练了。如果跨越边界的异常值在训练时不忽略，就会带来又大又困难的修正误差项，训练也不会收敛。在测试时，我们还是应用全卷积的RPN到整个图像中，这可能生成跨越边界的建议框，我们将其裁剪到图像边缘位置。有些RPN建议框和其他建议框大量重叠，为了减少冗余，我们基于建议区域的cls得分，对其采用非极大值抑制（non-maximum suppression, NMS）。我们固定对NMS的IoU阈值为0.7，这样每个图像只剩2k个建议区域。正如下面展示的，NMS不会影响最终的检测准确率，但是大幅地减少了建议框的数量。NMS之后，我们用建议区域中的top-N个来检测。在下文中，我们用2k个RPN建议框训练Fast R-CNN，但是在测试时会对不同数量的建议框进行评价。 4.实验各种对比实验，此处略过 5.总结本文提出了能够高效、准确生成区域推荐（Region Proposal）的RPNs，通过和下流检测网络共享卷积特征，Region Proposal 网络几乎没有计算消耗。本文的方法是能够使统一的基于深度学习的目标检测算法以接近实时的帧速运行在目标检测系统上。学习到的RPN能够提升区域推荐的质量从而提高整体目标检测的准确度。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast RCNN论文阅读笔记]]></title>
    <url>%2F2018%2F01%2F03%2FFast-RCNN%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文地址：https://arxiv.org/pdf/1504.08083.pdf项目地址：https://github.com/rbgirshick/fast-rcnn Fast RCNN是继2014年提出的RCNN之后，Ross Girshick在15年推出的，构思精巧，流程更为紧凑，大幅提升了目标检测的速度。同样使用最大规模网络，训练时间从84小时减为9.5小时，基于VGG16的Fast RCNN算法在训练速度上比RCNN快了将近9倍，比SPPnet快大概3倍；测试时间从47秒减少为0.32秒，测试速度比RCNN快了213倍，比SPPnet快了10倍。在VOC2012上的mAP在66%左右。 1.背景介绍Fast RCNN 是针对RCNN +SPPNet 的改进，改进的原因是：1.1.RCNN(1)训练是多级pipeline的。RCNN首先采用log代价函数在目标候选框上微调卷积网络，然后，将SVMs适用到卷积特征。SVMs取代了通过微调学习softmax分类器，充当了目标检测器的任务。在训练的第三阶段，学习检测框Bounding-box回归。(2)训练非常耗费空间和时间。对于SVM和Bounding-Box回归训练，针对每个图像的每个目标推荐的特征被提取，并被写到硬盘中。采用非常深的网络，如VGG16，这个过程大概会花费2.5个GPU-days，仅仅针对VOC2007训练集的5k图片。这些特征要求成百上千GB的存储空间。(3)目标检测非常的慢。在测试时，从每张测试图像提取的每个目标推荐的特征。在GPU上用VGG16来检测一张图片需要花费47s。由此可以看出RCNN的问题所在，首先在提取完proposal之后，整个网络对提取到的RCNN中的所有的proposal都进行了整套的提取特征这些操作，这些操作是非常耗时，耗费空间的。事实上我们并不需要对每个proposal都进行CNN操作，只需要对原始的整张图片进行CNN操作即可，因为我们所提取到的proposal属于整张图片，因此对整张图片提取出feature map之后，再找出对应proposal在feature map中对应的区域，进行对比分类即可。第二个问题所在就是在框架中2-3过程中的对提取到的区域进行变形，我们知道CNN提取特征的过程对图像的大小并无要求，只是在提取完特征，进行全连接的时候才需要固定尺寸的特征，然后使用SVM等分类器进行分类操作，当然这两个问题在SPPNET中得到了很好的解决。1.2.SPPNet 引入原因：在RCNN中，使用完ss提取proposal之后，对每个proposal都进行了CNN提取特征+SVM分类。解决方法：因为region proposal都是图像的一部分，我们只需要对图像提一次卷积层特征，然后将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。更直白的讲就是SPP-NET代替卷积网络中最后一个pooling层，而且这pooling层是多scale的。SPP_net 源于微软2014年发布的论文《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》，主要实现基于空间金字塔的分类与检测，该方法可以大大提高目标检测的速度，相比RCNN有60倍的性能提升。 SPP net中Region Proposal仍然是在原始输入图像中选取的，不过是通过CNN映射到了feature map中的一片区域。空间金字塔池化的思想是：对卷积层的feature map上Region Proposal映射的区域分别划分成1×1，2×2，4×4的窗口（window），并在每个窗口内做max pooling，这样对于一个卷积核产生的feature map，就可以由SPP得到一个（1×1+2×2+4×4）维的特征向量。论文中采用的网络结构最后一层卷积层共有256个卷积核，所以最后会得到一个固定维度的特征向量（1×1+2×2+4×4）×256维），并用此特征向量作为全连接层的输入后做分类。相比于R-CNN，SPP net是使用原始图像作为CNN网络的输入来计算feature map（R-CNN中是每个Region Proposal都要经历一次CNN计算），这样就大大减少了计算量。另外，SPP net中Region Proposal仍然是通过选择性搜索等算法在输入图像中生成的，通过映射的方式得到feature map中对应的区域，并对Region Proposal在feature map中对应区域做空间金字塔池化。通过空间金字塔池化操作，对于任意尺寸的候选区域，经过SPP后都会得到固定长度的特征向量。SPP net的优点：任意尺寸输入，固定尺寸输出。2.本文的贡献Fast RCNN针对RCNN在训练时是multi-stage pipeline和训练的过程中很耗费时间空间的问题进行改进。它主要是将深度网络和后面的SVM分类两个阶段整合到一起，使用一个新的网络直接做分类和回归。主要做了以下改进:(1)最后一个卷积层后加了一个ROI pooling layer。ROI pooling layer首先可以将image中的ROI定位到feature map，然后是用一个单层的SPP layer将这个feature map patch池化为固定大小的feature之后再传入全连接层。(2).损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练。 3.Fast RCNN结构和训练Fast RCNN的网络输入是整张图片和一组object proposals。网络首先用conv和max pooling处理整张图片生成整张图片的feature map。然后对于每一个object proposal，使用region of interest（RoI）polling层从feature map中抽取一个固定长度的特征向量。接下来将每一个特征向量通过两个全连接层得到最终的4096维的feature。分别送到softmax中去做分类和bbox regressor中做位置的回归。值得注意的是它把分类和回归也放在了CNN网络中来做，而不是像RCNN那样独立开来 3.1ROI Pooling LayerRoI(region of interest)是候选框（region proposal）映射到最后一层卷积层生成的。由于候选框的大小不同，映射到最后一层卷积层的RoI区域的大小也不同，这样就会导致特征提取的维度不同。因此需要通过RoI pooling得到固定长度的特征，以便输入到后面的全连接层中。RoI pooling的对象是输入图像中产生的proposal在feature map上的映射区域RoI pooling是这样的，它的输入是不同大小的feature map，那么为了得到相同大小的feature，怎么做呢？举个栗子，这两个窗口虽然不一样大，但是我都给它分成9份，然后在每个小区域做max pooling，这样都能得到9维的向量，这就是ROI pooling。 3.2.Initializing from pre-trained networks 预训练过程对网络修改主要有三个方面： -最后一个max pooling layer被RoI pooling layer代替。目的是将每个RoI对应的feature map resize到固定的大小（对于VGG16来说H与W都是7）使其能够传送到接下来的全连接层进行训练。-最后一个全连接层和softmax层被改成两个自层，分别是softmax做分类和bbox regression做回归。-网络的输入包含两个部分：图片和这些图片对应的RoIs。 3.3.Fine-tuning for detection 使用BP算法训练网络是Fast R-CNN的重要能力，前面已经说过，SPP-net不能微调spp层之前的层，主要是因为当每一个训练样本来自于不同的图片时，经过SPP层的BP算法是很低效的（感受野太大）。这正是训练RCNN和SPPNet网络的方法，低效的部分是因为每个ROI可能具有非常大的感受野，通常跨越整个输入图像。由于反向传播必须处理整个感受野，训练输入很大（通常是整个图像）。提出了一种更有效的训练方法，利用训练期间的特征共享。 Fast R-CNN提出SGD mini_batch分层取样的方法：首先随机取样N张图片，然后每张图片取样R/N个RoIs e.g. N=2 and R=128 除了分层取样，还有一个就是FRCN在一次微调中联合优化softmax分类器和bbox回归，看似一步，实际包含了多任务损失（multi-task loss）、小批量取样（mini-batch sampling）、RoI pooling层的反向传播（backpropagation through RoI pooling layers）、SGD超参数（SGD hyperparameters）。分层采样在Fast RCNN网络训练中，随机梯度下降（SGD）的小批量是被分层采样的，首先采样N个图像，然后从每个图像采样R/N个 RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减N，就减少了小批量的计算。例如，当N=2和R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。更加精细的训练过程在微调阶段联合优化Softmax分类器和检测框回归，而不是分别在三个独立的阶段训练softmax分类器，SVM和回归器。Multi-task loss多任务一起训练自然就需要定义一个多任务的误差函数.softmax的输出 p = (p_0,...,p_k)代表了K+1类对应的概率.对每一个类k,bbox regressor的输出是 t^k = (t^k_x,t^k_y,t^k_w,t^k_h)这跟R-CNN是一样的.多任务误差函数由两个部分组成.第一部分是分类误差(ground truth类的log误差) L_{cls}(p,u) = -log p_u第二部分是定位误差(bbox regressor的误差) {\lambda}[u \geq1]L_{loc}(t^u,v)对于u=0的,代表背景的RoI,由于背景并不存在真实位置,所以这部分的定位误差被忽略.而对于u&gt;=1的其他类,使用如下误差 L_{loc}(t^u,v)= \sum_{i\in\{x,y,w,h\}} smooth_{L1}(t^u_i - v_i)其中 smooth_{L1}(x)= \begin{cases} 0.5x^2\ \ if|x|]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN算法笔记]]></title>
    <url>%2F2017%2F12%2F24%2FRCNN%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文地址：https://arxiv.org/pdf/1311.2524.pdf项目地址：https://github.com/rbgirshick/rcnnRCNN 可以说是采用深度学习来解决目标检测任务的开山之作。发表于2014年发表于计算机视觉三大顶会之一CVPR上，作者Ross Girshick多次在PASCAL VOC 的目标检测竞赛中获得很好的成绩，是继DPM方法多年平台期过后，取得显著效果的一篇文章。 RCNN的主要贡献是将CNN方法引入到了目标检测领域，大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路。然后由Ross Girshick、Kaiming He、Shaoqing Ren和一系列工作人员在此基础上进行拓展，出现了一系列的two stage的基于Region Proposal 采用深度学习解决目标检测问题的一系列算法：Fast RCNN、 Faster RCNN、PVA-Faster RCNN等，代表了当下目标检测方法的一个前沿分支。论文的主要贡献1.将卷积神经网络应用Region Proposal的策略，自底向上来进行定位目标和图像分割。2.当标注数据比较稀疏时，在有监督的数据集上训练到特定任务数据集上fine-tuning可以取得较好性能。效果比当时最好的DPM算法 mAP高20点，在VOC上取得了最好的性能。 论文主要特点（比较于传统方法）1.速度：经典的目标检测算法使用滑动窗口法依次判断所有可能的区域。本文则采用（Selective Search方法）预先提取一系列较可能的物体候选区域，之后在这些候选区域上采用CNN进行特征提取，判断目标。2.训练集：经典的目标检测算法在区域中提取人工设定的特征。本文采用深度学习进行特征提取。使用了两个数据库：一个较大的识别数据库（ImageNet ILSVC 2012），其标定了每张图片中个物体数据的类别。一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置，20类。本文使用识别库进行预训练得到CNN（有监督预训练），然后采用检测库进行参数调优训练，最后在检测库上评测。 RCNN 算法基本流程1.一张图像自底向上生成2k个左右候选区域2.对每个候选区域，使用深度学习网络进行特征提取3.特征送入每一类的SVM分类器，判别是否属于该类4.使用回归器精细修正候选框位置Region Proposal（候选区域生成）使用 Selective Search 方法从一张图像生成约2k个候选区域。基本思路：1.使用一种过分割手段，将图像分割成小区域2.查看现有小区域，合并可能性最高的两个区域。重复直到整张图像合并成一个区域位置。3.输出所有曾经存在过的区域，即候选区域。候选区域和后续步骤相对独立，可采用任意算法进行。合并规则优先合并以下四种区域：-颜色（颜色直方图）相近的-纹理（梯度直方图）相近的-合并后总面积小的-合并后，总面积在其BBOX中所占比例大的。 第三条，保证合并操作的尺度比较均匀，避免一个大区域陆续“吃掉”其他小区域。例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh。第四条，保证合并后形状规则。上述四条规则只涉及区域的颜色直方图、梯度直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。有监督预训练与无监督预训练 (1)无监督预训练(Unsupervised pre-training) 预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。 (2)有监督预训练(Supervised pre-training) 所谓的有监督预训练也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务时：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样相比于你直接采用随机初始化的方法，精度可以有很大的提高。 对于目标检测问题： 图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇论文采用了迁移学习的思想： 先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片分类训练。这个数据库有大量的标注数据，共包含了1000种类别物体，因此预训练阶段CNN模型的输出是1000个神经元（当然也直接可以采用Alexnet训练好的模型参数）。重叠度（IOU）: 物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示就是矩形框A、B的重叠面积占A、B并集的面积比例。非极大值抑制（NMS）： RCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框为做类别分类概率：就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于车辆的概率 分别为A、B、C、D、E、F。 (1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值; (2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。 (3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。 就这样一直重复，找到所有被保留下来的矩形框。 非极大值抑制（NMS）顾名思义就是抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。这里不讨论通用的NMS算法，而是用于在目标检测中用于提取分数最高的窗口的。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。 VOC物体检测任务相当于一个竞赛，里面包含了20个物体类别：PASCAL VOC2011 Example Images 还有一个背景，总共就相当于21个类别，因此一会设计fine-tuning CNN的时候，我们softmax分类输出层为21个神经元。总体思路再回顾：首先对每一个输入的图片产生近2000个不分种类的候选区域（region proposals），然后使用CNNs从每个候选框中提取一个固定长度的特征向量（4096维度），接着对每个取出的特征向量使用特定种类的线性SVM进行分类。也就是总个过程分为三个程序：a、找出候选框；b、利用CNN提取特征向量；c、利用SVM进行特征向量分类。候选框搜索阶段： 当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法： (1)各向异性缩放 这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227*227，如下图(D)所示；(2)各向同性缩放 因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案。有两种办法 A、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示; B、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示; 对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。 （备注：候选框的搜索策略作者也考虑过使用一个滑动窗口的方法，然而由于更深的网络，更大的输入图片和滑动步长，使得使用滑动窗口来定位的方法充满了挑战。） CNN特征提取阶段： 1、算法实现 a、网络结构设计阶段 网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ） 参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这里文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。网络优化求解时采用随机梯度下降法，学习率大小为0.001；C、fine-tuning阶段 （图片数据库： PASCAL VOC） 我们接着采用 selective search 搜索出来的候选框 （PASCAL VOC 数据库中的图片） 继续对上面预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景) (20 + 1bg = 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个事正样本、96个事负样本。关于正负样本问题： 一张照片我们得到了2000个候选框。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此在CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框（PASCAL VOC的图片都有人工标注）的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本）。（ 如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了） 疑惑点： CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，在训练的时候最后一层softmax就是分类层。那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？ 这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm，具体请看下文。 SVM训练、测试阶段 训练阶段： 这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到20004096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background) 。 排序，canny边界检测之后就得到了我们需要的bounding-box。 再回顾总结一下：整个系统分为三个部分：1.产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标2.一个大的卷积神经网络，对每个region产生一个固定长度的特征向量3.一系列特定类别的线性SVM分类器。位置精修： 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。测试阶段： 使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后再CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数，再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(then B-BoxRegression)。 （非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类）。作者提到花费在region propasals和提取特征的时间是13s/张-GPU和53s/张-CPU，可以看出时间还是很长的，不能够达到及时性。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNets论文阅读笔记]]></title>
    <url>%2F2017%2F12%2F23%2FMobileNets%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[移动和嵌入式设备视觉应用的高效卷积神经网络模型：MobileNets论文地址：https://arxiv.org/abs/1704.04861 摘要本文针对移动和嵌入式视觉设备提出一类被称作MobileNets的高效模型。MobileNets是一个使用深度可分解卷积（depthwise separable convolutions）来构建轻量级深度神经网络的精简结构（streamlined architecture，流线型结构 or 精简结构，倾向于后者）。本文引入了两个 简单的全局超参来有效权衡延迟（latency）和准确度（accuracy）。这些超参允许模型构建者根据具体问题的限制为他们的应用选择规模合适的模型。在资源和准确度的权衡条件下进行广泛的实验，与ImageNet分类任务的其它主流模型相比较，本文的模型显示出很好的性能。然后 ，本文在一系列的应用和用例下验证了其有效性，包括：目标检测、细粒度分类、人脸属性提取和 大规模地理定位。 1.引言 自从AlexNet在ILSVRC2012利用深度卷积神经网络赢得ImageNet挑战赛以来，卷积神经网络（CNN）在计算机视觉领域被广泛使用。这方面的应用主流趋势是采用更深、更复杂的网络实现更高的精度。但是，考虑到模型大小和速度。精度的提升并不一定使网络更加高效。在很多实际的应用场景，如机器人、自动驾驶和 增强现实，这些识别任务需要在计算资源限制的平台实时地运行。 本文提出了一个高效的网络结构和一组两个超参，用以构建较小的、低延迟模型，从而能比较好的满足移动和 嵌入式视觉应用的设计要求。 2.背景介绍在最近的文献中，关于建立小型高效的神经网络的研究日益增加。一般说来，这些方法可以归为两类，压缩预训练模型和直接训练小网络模型。本文提出了一类允许模型开发人员选择与其应用程序的资源限制（延迟，大小）相匹配的小型网络架构。MobileNets主要侧重于优化延迟，但也能够产生小型网络。很多关于小型网络的论文只关注大小，却不考虑速度问题。记录：需要注意的是目前在应用于移动和嵌入式设备的深度学习 网络研究主要有两个方向：1）直接设计较小且高效的网络，并训练。本文属于这方面，另外比较经典的如GoogleNet提出Inception思想，采用小卷积。Network in Network 改进传统CNN，使用1x1卷积，提出MLP CONV层Deep Fried Convents采用Adaptive Fast-food transform重新 参数化全连接层的向量矩阵。SqueezeNet设计目标主要是为了简化CNN的模型参数数量，主要采用了替换卷积核3x3为1x1，使用了deep compression技术对网络进行了压缩 。Flattened networks针对快速前馈执行设计的扁平神经网络。2）对预训练模型进行压缩小、分解或者压缩。采用hashing trick进行压缩；采用huffman编码；用大网络来教小网络；训练低精度乘法器；采用二进制输入二进制权值等等。 3.MobileNet 架构首先描述了使用深度可分解滤波（depth wise separable filters）建立的MobileNets核心层；然后介绍了MobileNet的网络结构，并且总结了两个模型收缩超参：宽度乘法器和分辨率乘法器（width multiplier和resolution multiplier）。3.1.深度可分解卷积（Depthwise Separable Convolution）MobileNet模型机遇深度可分解卷积，其可以将标准卷积分解成 一个深度卷积和一个1x1的点卷积。3.2. 网络结构和训练3.3.宽度乘法器（Width Multiplier）3.4. 分辨率乘法器（Resolution Multiplier） 此部分待更新 4.实验 本节首先调查了深度卷积（ depthwise convolution）的影响，以及通过减小网络宽度 而不是减少层数来选择压缩的 模型。然后基于两个超参：宽度乘法器和分辨率乘法器（width multiplier和resolution multiplier）进行网络收缩，并把其和现阶段主流的模型进行比较。研究结果表明MobileNet可以应用于许多不同的任务。4.1模型选择 首先比较了深度可分解卷积的MobileNet和全卷积的模型，如表4所示 ，使用深度可分解 卷积和全卷积相比，在ImageNet的精确度只下降了1%，但是Mult-Adds和参数大大节省。 表5显示，在计算和参数数量相似时，更浅的模型比更小的模型精度低3%。 4.2.模型超参收缩 表6显示宽度乘法器（width multiplier）超参α减小时，模型准确率随模型的变小而下降。 表7 显示分辨率乘法器（resolution multiplier）超参ρ 减小时，模型准确率随模型的分辨率变小而降低。 表8中将MobileNet和GoogleNet、VGG16进行了比较。表9比较了两个超参的变化4.3.细粒度识别任务本文在Stanford Dogs数据集上训练了MobileNet以应对细粒度识别任务。本文扩展了指定对比方法［19］并且收集了一个更大且包含更多噪声的数据集超过了指定方法［18］的规模。采用了网络噪声数据去预训练细粒度分类狗的 识别模型，然后在Stanford Dogs训练集上进行微调。结果如表10。MobileNet能够在极大的较少计算和尺寸的情况下获得接近于［18］的结果。4.4.大规模地理定位PlaNet的任务是用于确定在一张照片在哪个地理位置进行拍摄的分类问题。该方法将地球划分进一个个网格单元集合到目标类别，用数百万计的有地理位置标记的图片训练卷积 神经网络。PlaNet已经能够成功对各种各样的照片进行地理位置标记，并且处理相同任务性能超过了Im2GPS。在相同的数据上采用MobileNet架构重新训练PlaNet，其结果如表11所示，MobileNet版本和PlaNet相比，规模小了 很多，性能只降低了很少，据此来说，其仍然超过了Im2GPS4.5.人脸属性提取 MobileNet的另一个应用场景是压缩具有未知且复杂训练程序的大型系统。在人脸属性分类任务中，我们证明了MobileNet和distillation间的协同关系，一种针对深度网络的知识迁移技术。我们试图去简化一个具有7500万参数和16亿Mult-Adds大型人脸属性分类器。该分类器在一个类似于YFCC100M的多属性数据集上进行训练。采用MobileNet架构去提取一个人脸属性分类器。distillation是通过训练分类器模型一个更大的模型输出，而不是采用人工标注，因而能够从大型（无限大潜在可能）未标注数据集训练。结合distillation的可扩展性和MobileNet的简洁参数化，终端系统不仅不要求正则化，而去反而表现出更好的性能。如表12。4.6.目标检测MobileNet也能够作为一个高效的基网络被部署到现代目标检测系统。基于最近2016 COCO挑战赛的获胜者的工作，我们应对目标检测任务在COCO数据集上训练了MobileNet，并进行了比较。表13列出了在Faster RCNN和SDD框架下，MobileNet、VGG以及Inception V2的比较。实验中，SSD以300的输入分辨率于分别是300和600输入分辨率的 Faster RCNN进行比较，在两个框架下，Mobile Net性能不低于其它两个网络结果，且计算复杂性和模型都相对更小。 记录：此处感觉采用VOT Challenge 的数据集能够更有效客观的进行目标检测的结果评测，不知道为什么采用了COCO。 4.7.Face EmbeddingsFaceNet是目前state-of-art的人脸识别模型，它基于triplet loss建立face Embedding。为了建立移动FaceNet模型，我们采用distillation通过最小化FaceNet和MobileNet在训练数据上的平方差（squared differences ）来训练。表15可以看到非常小的MobileNet模型结果。 5.总结本文提出了一个基于深度可分解卷积（depthwise separable convolutions）的新模型架构MobileNets。分析了 决定高效模型的重要设计思路。然后，本文讲解了如何使用宽度乘法器（width multiplier）和分辨率乘法器（resolution multiplier），通过权衡较为可靠的精确度来减小尺寸大小和延迟时间，来构建更小更快的MobileNets。将不同的MobileNets和主流的模型进行比较，展现了 MobileNets在大小、速度和精确度这些特性都具有明显优势。最后，我们通过一系列任务的应用，验证了MobileNets的广泛适用性。下一步，本文的计划是在TensorFlow发布他们的模型。 记录：MobileNet应该是目前在设计小网络方向性能比较比较比较好的论文，论文中并看不出其具体的fps的数据，也是论文的缺憾，不过预计应该会有不错的性能。其也是深度学习网络应用到嵌入式和移动设备的一个比较好的参照。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>嵌入式神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云服务器搭建SVN版本控制工具]]></title>
    <url>%2F2017%2F12%2F23%2F%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BASVN%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[阿里云服务器搭建SVN版本控制工具记录在终端下采用ssh root@47.92.3.188连接我的服务器进入远程主机操作修改阿里云主机名称命令：hostnamectl set-hostname 新主机名 SVN部署操作检查是否安装地板本SVN：rpm -qa subversion卸载旧版本svn:yum remove subversion安装svn服务器端：yum install subversion执行以下命令：cd /usr/local 进入目录mkdir svnRepo 创建SVN目录chmod -R 777 svnRepo 修改权限777svnadmin create /usr/local/svnRepo/test_server 创建一个svn版本仓库test_server(test_server可以随便取名字） cd svnRepo/test_server/conf 1）修改该目录下三个配置文件vi svnserve.conf把 anon-access = readauth-access = writepassword-db= passwdrealm = test_server前#号和空格去掉，变成anon-access = none //修改成noneauth-access = writepassword-db= passwdrealm = test_server //改成自己的版本库保存退出2）vi authz //文件，创建svn组和组用户的权限test_server //创建test_server组，并制定三个用户whl，wxr，ryg[/] //制定根目录下的权限[test_server:/]//制定版本分支目录下的权限@test_server = rw // test_server组用户权限为读写× = r //其他用户只有读权限保存退出3）修改或创建用户密码vi passwd[users]whl = Jitu2017wxr = Jitu2017ryg = Jitu2017保存退出 设置自启动vi /etc/rc.local //打开自启动文件 文件内容如下 !/bin/bashTHIS FILE IS ADDED FOR COMPATIBILITY PURPOSES# It is highly advisable to create own systemd services or udev rulesto run scripts during boot instead of using this file.# In contrast to previous versions due to parallel execution during bootthis script will NOT be run after all other services.# Please note that you must run ‘chmod +x /etc/rc.d/rc.local’ to ensurethat this script will be executed during boot.touch /var/lock/subsys/local 添加下面一行svnserve -d -r /usr/local/svnRepo/avalon_server保存退出ps aux | grep ‘svn’ 查找所有svn启动的进程杀死，然后启动svn svnserve -d -r /usr/local/svnRepo/first 启动svn(可以把这个放到/etc/local/rc.local文件中，实现开机自启动)sudo netstat -anp | grep svnserve //验证是否开启成功 关闭svnservesudo pstree | grep svn #查看 sudo killall svnserve #关闭 SVN版本库起动方式: 1：单版本库起动 svnserve -d -r /usr/local/svnRepo/test_server2：多版本库起动 svnserve -d -r /usr/local/svnRepo区别在于起动svn时候的命令中的启动参数-r指定的目录。 连接 svn://47.92.3.188:3690命令行下采用svn checkout svn://47.92.3.188:3690 已经过iOS PHP等项目版本控制验证]]></content>
      <categories>
        <category>版本控制</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PVA Faster RCNN配置安装及使用]]></title>
    <url>%2F2017%2F12%2F22%2FPVA-Faster-RCNN%E9%85%8D%E7%BD%AE%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[论文：PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection简介PVANET是目前目标检测算法比较好的实现，也是Region Proposal系列方法的一个新实现，目前达到了real-time的单张图像40ms，且在VOC12上达到了精度83.8%，是region proposal目前精度最高的，也突破了region proposal系列方法RCNN、Fast RCNN、Faster RCNN的精度高不能实时的问题。目前回归类方法最新的SSD以及YOLO9000虽然速度很快，但是精度比其差了不少。所以PVANET还是值得实现的。论文地址：https://www.arxiv.org/pdf/1608.08021v3.pdfgithub项目:https://github.com/sanghoon/pva-faster-rcnn实现环境操作系统：Ubuntu 16.04LTS显卡：NVIDIA GTX 1080TICUDA8.0Cudnn V5.1其它配置忽略（不重要） Caffe General Dependencies安装 sudo apt-get update sudo apt-get upgrade sudo apt-get install -y build-essential cmake git pkg-config sudo apt-get install -y libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install -y libatlas-base-dev sudo apt-get install -y --no-install-recommends libboost-all-dev sudo apt-get install -y libgflags-dev libgoogle-glog-dev liblmdb-dev (Python general) sudo apt-get install -y python-pip \#(Python 2.7 development files) sudo apt-get install -y python-dev sudo apt-get install -y python-numpy python-scipy OpenCV安装此处根据git master branch安装的3.2.0-dev版本 \#Build tools: sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev cd ~ git clone https://github.com/opencv/opencv.git cd opencv mkdir build cd build cmake -D CMAKE_BUILD_TYPE=RELEASE \ -D CMAKE_INSTALL_PREFIX=/usr/local \ -D WITH_CUDA=ON \ -D WITH_CUBLAS=1 \ -D INSTALL_PYTHON_EXAMPLES=ON \ -D BUILD_EXAMPLES=ON .. make all -j16（此处根据你自己的计算机性能进行安装） sudo make -j16 install sudo ldconfig 检查安装 $ python \>>>import cv2 \>>>cv2.__version__ 输出：'3.2.0-dev' 有关CUDA 8.0+Cudnn5.1的安装请自行百度 pva-faster-rcnn的搭建 1.获取项目 git clone --recursive https://github.com/sanghoon/pva-faster-rcnn.git 2.编译建立Cython模块 安装python依赖 sudo pip install Cython sudo pip install easydict cd pva-faster-rcnn/lib 此处需要修改lib下的setup.py第135行 GPU 计算能力查看地址'nvcc': ['-arch=sm_35', https://developer.nvidia.com/cuda-gpus ![](http://upload-images.jianshu.io/upload_images/3478042-7250cd061ba5e39a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 比如我的是1080TI，计算能力为6.1，则修改为 'nvcc': ['-arch=sm_61', 执行 make all -j16 进行编译 3.编译建立Caffe和pycaffe cd pva-faster-rcnn/caffe-fast-rcnn cp Makefile.config.example Makefile.config gedit Makefile.config 修改Makefile.config文件: 需要修改的行如下 USE_CUDNN := 1 OPENCV_VERSION := 3 #CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda CUDA_DIR := /usr/local/cuda-8.0 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-Linux-gnu/hdf5/serial/ INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-Linux-gnu/hdf5/serial/ #Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 #Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) USE_PKG_CONFIG := 1 Makefile修改 cd ~/pva-faster-rcnn/caffe-faster-rcnn gedit Makefile 需要注意的是暂时不要采用最新的cudnn v6版本，我使用的时候发觉会出错，v5.1版本修改此配置。 原有 NVCCFLAGS += -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) 修改为 NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) 更新caffe-fast-rcnn的caffe部分，因为此项目caffe不是最新涉及到cudnn的计算会报错。 # Make sure to clone with --recursive cd ~/pva-faster-rcnn/caffe-fast-rcnn git remote add -f caffe https://github.com/BVLC/caffe.git git merge -X theirs caffe/master gedit include/caffe/layers/python_layer.hpp \# Remove self_.attr("phase") = static_cast(this->phase_); cd ~/pva-faster-rcnn/caffe-faster-rcnn/python for req in $(cat requirements.txt); do sudo -H pip install $req --upgrade; done 编译安装 mkdir build cd build cmake .. make -j16 all make -j16 pycaffe make -j16 install Caffe路径设置 gedit ~/.bashrc export CAFFE_ROOT=~/py-faster-rcnn/caffe-fast-rcnn export PYTHONPATH=~/py-faster-rcnn/caffe-fast-rcnn/python:$PYTHONPATH source ~/.bashrc 检查安装 $ python >>> import caffe >>> caffe.__version__ sudo -H pip install easydict sudo apt-get install python-gi-cairo python-tk 4.下载预训练模型如果有VPN网速好的话直接采用pva-faster-rcnn/models下的的shell脚本进行下载此处采用百度网盘下载链接：http://pan.baidu.com/s/1kVRRPDd 密码：1cdt1、打开文件将test.model放入$pva-faster-rcnn/models/pvanet/full/这个目录下2、将test(1).model重命名为test.model放入$pva-faster-rcnn/models/pvanet/comp/目录下5.voc2007数据集下载打开终端（任何目录）输入： wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar 解压（严格按照此顺序）： tar xvf VOCtrainval_06-Nov-2007.tar tar xvf VOCtest_06-Nov-2007.tar tar xvf VOCdevkit_08-Jun-2007.tar 将生成的VOCdevkit文件夹更名为VOCdevkit2007移动至$pva-faster-rcnn/data/目录下面6.测试需要注意的是缺少test.pt，请将pvanet_obsolete下的对应文件复制过来即可终端输入： cd $pva-faster-rcnn 1、full/test.model测试： ./tools/test_net.py --gpu 0 --def models/pvanet/full/test.pt --net models/pvanet/full/test.model --cfg models/pvanet/cfgs/submit_0716.yml 2、Comp/test.model测试： ./tools/test_net.py --gpu 0 --def models/pvanet/comp/test.pt --net models/pvanet/comp/test.model --cfg models/pvanet/cfgs/submit_0716.yml 此测试会得到系列类别的AP值3.模型的可视化检测demo此处需要重新编写demo.py文件可在我github下fork的分支获取该文件demo.py文件https://github.com/YgRen/pva-faster-rcnn cd pva-faster-rcnn 执行 ./tools/demo.py --gpu 0 --def models/pvanet/comp/test.pt --net models/pvanet/comp/test.model 生成可视化结果 记录：关于自定义数据集的训练和调参后期会进行发布，关于VOC07和12数据集的训练，直接参照github项目地址操作即可，最近会更新论文精读。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
