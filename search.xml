<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MobileNets论文阅读笔记]]></title>
    <url>%2F2017%2F12%2F23%2FMobileNets%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[移动和嵌入式设备视觉应用的高效卷积神经网络模型：MobileNets论文地址：https://arxiv.org/abs/1704.04861 摘要本文针对移动和嵌入式视觉设备提出一类被称作MobileNets的高效模型。MobileNets是一个使用深度可分解卷积（depthwise separable convolutions）来构建轻量级深度神经网络的精简结构（streamlined architecture，流线型结构 or 精简结构，倾向于后者）。本文引入了两个 简单的全局超参来有效权衡延迟（latency）和准确度（accuracy）。这些超参允许模型构建者根据具体问题的限制为他们的应用选择规模合适的模型。在资源和准确度的权衡条件下进行广泛的实验，与ImageNet分类任务的其它主流模型相比较，本文的模型显示出很好的性能。然后 ，本文在一系列的应用和用例下验证了其有效性，包括：目标检测、细粒度分类、人脸属性提取和 大规模地理定位。 1.引言 自从AlexNet在ILSVRC2012利用深度卷积神经网络赢得ImageNet挑战赛以来，卷积神经网络（CNN）在计算机视觉领域被广泛使用。这方面的应用主流趋势是采用更深、更复杂的网络实现更高的精度。但是，考虑到模型大小和速度。精度的提升并不一定使网络更加高效。在很多实际的应用场景，如机器人、自动驾驶和 增强现实，这些识别任务需要在计算资源限制的平台实时地运行。 本文提出了一个高效的网络结构和一组两个超参，用以构建较小的、低延迟模型，从而能比较好的满足移动和 嵌入式视觉应用的设计要求。 2.背景介绍在最近的文献中，关于建立小型高效的神经网络的研究日益增加。一般说来，这些方法可以归为两类，压缩预训练模型和直接训练小网络模型。本文提出了一类允许模型开发人员选择与其应用程序的资源限制（延迟，大小）相匹配的小型网络架构。MobileNets主要侧重于优化延迟，但也能够产生小型网络。很多关于小型网络的论文只关注大小，却不考虑速度问题。记录：需要注意的是目前在应用于移动和嵌入式设备的深度学习 网络研究主要有两个方向：1）直接设计较小且高效的网络，并训练。本文属于这方面，另外比较经典的如GoogleNet提出Inception思想，采用小卷积。Network in Network 改进传统CNN，使用1x1卷积，提出MLP CONV层Deep Fried Convents采用Adaptive Fast-food transform重新 参数化全连接层的向量矩阵。SqueezeNet设计目标主要是为了简化CNN的模型参数数量，主要采用了替换卷积核3x3为1x1，使用了deep compression技术对网络进行了压缩 。Flattened networks针对快速前馈执行设计的扁平神经网络。2）对预训练模型进行压缩小、分解或者压缩。采用hashing trick进行压缩；采用huffman编码；用大网络来教小网络；训练低精度乘法器；采用二进制输入二进制权值等等。 3.MobileNet 架构首先描述了使用深度可分解滤波（depth wise separable filters）建立的MobileNets核心层；然后介绍了MobileNet的网络结构，并且总结了两个模型收缩超参：宽度乘法器和分辨率乘法器（width multiplier和resolution multiplier）。3.1.深度可分解卷积（Depthwise Separable Convolution）MobileNet模型机遇深度可分解卷积，其可以将标准卷积分解成 一个深度卷积和一个1x1的点卷积。3.2. 网络结构和训练3.3.宽度乘法器（Width Multiplier）3.4. 分辨率乘法器（Resolution Multiplier） 此部分待更新 4.实验 本节首先调查了深度卷积（ depthwise convolution）的影响，以及通过减小网络宽度 而不是减少层数来选择压缩的 模型。然后基于两个超参：宽度乘法器和分辨率乘法器（width multiplier和resolution multiplier）进行网络收缩，并把其和现阶段主流的模型进行比较。研究结果表明MobileNet可以应用于许多不同的任务。4.1模型选择 首先比较了深度可分解卷积的MobileNet和全卷积的模型，如表4所示 ，使用深度可分解 卷积和全卷积相比，在ImageNet的精确度只下降了1%，但是Mult-Adds和参数大大节省。 表5显示，在计算和参数数量相似时，更浅的模型比更小的模型精度低3%。 4.2.模型超参收缩 表6显示宽度乘法器（width multiplier）超参α减小时，模型准确率随模型的变小而下降。 表7 显示分辨率乘法器（resolution multiplier）超参ρ 减小时，模型准确率随模型的分辨率变小而降低。 表8中将MobileNet和GoogleNet、VGG16进行了比较。表9比较了两个超参的变化4.3.细粒度识别任务本文在Stanford Dogs数据集上训练了MobileNet以应对细粒度识别任务。本文扩展了指定对比方法［19］并且收集了一个更大且包含更多噪声的数据集超过了指定方法［18］的规模。采用了网络噪声数据去预训练细粒度分类狗的 识别模型，然后在Stanford Dogs训练集上进行微调。结果如表10。MobileNet能够在极大的较少计算和尺寸的情况下获得接近于［18］的结果。4.4.大规模地理定位PlaNet的任务是用于确定在一张照片在哪个地理位置进行拍摄的分类问题。该方法将地球划分进一个个网格单元集合到目标类别，用数百万计的有地理位置标记的图片训练卷积 神经网络。PlaNet已经能够成功对各种各样的照片进行地理位置标记，并且处理相同任务性能超过了Im2GPS。在相同的数据上采用MobileNet架构重新训练PlaNet，其结果如表11所示，MobileNet版本和PlaNet相比，规模小了 很多，性能只降低了很少，据此来说，其仍然超过了Im2GPS4.5.人脸属性提取 MobileNet的另一个应用场景是压缩具有未知且复杂训练程序的大型系统。在人脸属性分类任务中，我们证明了MobileNet和distillation间的协同关系，一种针对深度网络的知识迁移技术。我们试图去简化一个具有7500万参数和16亿Mult-Adds大型人脸属性分类器。该分类器在一个类似于YFCC100M的多属性数据集上进行训练。采用MobileNet架构去提取一个人脸属性分类器。distillation是通过训练分类器模型一个更大的模型输出，而不是采用人工标注，因而能够从大型（无限大潜在可能）未标注数据集训练。结合distillation的可扩展性和MobileNet的简洁参数化，终端系统不仅不要求正则化，而去反而表现出更好的性能。如表12。4.6.目标检测MobileNet也能够作为一个高效的基网络被部署到现代目标检测系统。基于最近2016 COCO挑战赛的获胜者的工作，我们应对目标检测任务在COCO数据集上训练了MobileNet，并进行了比较。表13列出了在Faster RCNN和SDD框架下，MobileNet、VGG以及Inception V2的比较。实验中，SSD以300的输入分辨率于分别是300和600输入分辨率的 Faster RCNN进行比较，在两个框架下，Mobile Net性能不低于其它两个网络结果，且计算复杂性和模型都相对更小。 记录：此处感觉采用VOT Challenge 的数据集能够更有效客观的进行目标检测的结果评测，不知道为什么采用了COCO。 4.7.Face EmbeddingsFaceNet是目前state-of-art的人脸识别模型，它基于triplet loss建立face Embedding。为了建立移动FaceNet模型，我们采用distillation通过最小化FaceNet和MobileNet在训练数据上的平方差（squared differences ）来训练。表15可以看到非常小的MobileNet模型结果。 5.总结本文提出了一个基于深度可分解卷积（depthwise separable convolutions）的新模型架构MobileNets。分析了 决定高效模型的重要设计思路。然后，本文讲解了如何使用宽度乘法器（width multiplier）和分辨率乘法器（resolution multiplier），通过权衡较为可靠的精确度来减小尺寸大小和延迟时间，来构建更小更快的MobileNets。将不同的MobileNets和主流的模型进行比较，展现了 MobileNets在大小、速度和精确度这些特性都具有明显优势。最后，我们通过一系列任务的应用，验证了MobileNets的广泛适用性。下一步，本文的计划是在TensorFlow发布他们的模型。 记录：MobileNet应该是目前在设计小网络方向性能比较比较比较好的论文，论文中并看不出其具体的fps的数据，也是论文的缺憾，不过预计应该会有不错的性能。其也是深度学习网络应用到嵌入式和移动设备的一个比较好的参照。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
        <tag>嵌入式神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云服务器搭建SVN版本控制工具]]></title>
    <url>%2F2017%2F12%2F23%2F%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BASVN%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[阿里云服务器搭建SVN版本控制工具记录在终端下采用ssh root@47.92.3.188连接我的服务器进入远程主机操作修改阿里云主机名称命令：hostnamectl set-hostname 新主机名 SVN部署操作检查是否安装地板本SVN：rpm -qa subversion卸载旧版本svn:yum remove subversion安装svn服务器端：yum install subversion执行以下命令：cd /usr/local 进入目录mkdir svnRepo 创建SVN目录chmod -R 777 svnRepo 修改权限777svnadmin create /usr/local/svnRepo/test_server 创建一个svn版本仓库test_server(test_server可以随便取名字） cd svnRepo/test_server/conf 1）修改该目录下三个配置文件vi svnserve.conf把 anon-access = readauth-access = writepassword-db= passwdrealm = test_server前#号和空格去掉，变成anon-access = none //修改成noneauth-access = writepassword-db= passwdrealm = test_server //改成自己的版本库保存退出2）vi authz //文件，创建svn组和组用户的权限test_server //创建test_server组，并制定三个用户whl，wxr，ryg[/] //制定根目录下的权限[test_server:/]//制定版本分支目录下的权限@test_server = rw // test_server组用户权限为读写× = r //其他用户只有读权限保存退出3）修改或创建用户密码vi passwd[users]whl = Jitu2017wxr = Jitu2017ryg = Jitu2017保存退出 设置自启动vi /etc/rc.local //打开自启动文件 文件内容如下 #!/bin/bash THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES# It is highly advisable to create own systemd services or udev rulesto run scripts during boot instead of using this file.# In contrast to previous versions due to parallel execution during bootthis script will NOT be run after all other services.# Please note that you must run ‘chmod +x /etc/rc.d/rc.local’ to ensurethat this script will be executed during boot.touch /var/lock/subsys/local 添加下面一行svnserve -d -r /usr/local/svnRepo/avalon_server保存退出ps aux | grep ‘svn’ 查找所有svn启动的进程杀死，然后启动svn svnserve -d -r /usr/local/svnRepo/first 启动svn(可以把这个放到/etc/local/rc.local文件中，实现开机自启动)sudo netstat -anp | grep svnserve //验证是否开启成功 关闭svnservesudo pstree | grep svn #查看 sudo killall svnserve #关闭 SVN版本库起动方式: 1：单版本库起动 svnserve -d -r /usr/local/svnRepo/test_server2：多版本库起动 svnserve -d -r /usr/local/svnRepo区别在于起动svn时候的命令中的启动参数-r指定的目录。 连接 svn://47.92.3.188:3690命令行下采用svn checkout svn://47.92.3.188:3690 已经过iOS PHP等项目版本控制验证]]></content>
      <categories>
        <category>版本控制</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PVA Faster RCNN配置安装及使用]]></title>
    <url>%2F2017%2F12%2F22%2FPVA-Faster-RCNN%E9%85%8D%E7%BD%AE%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[论文：PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection简介PVANET是目前目标检测算法比较好的实现，也是Region Proposal系列方法的一个新实现，目前达到了real-time的单张图像40ms，且在VOC12上达到了精度83.8%，是region proposal目前精度最高的，也突破了region proposal系列方法RCNN、Fast RCNN、Faster RCNN的精度高不能实时的问题。目前回归类方法最新的SSD以及YOLO9000虽然速度很快，但是精度比其差了不少。所以PVANET还是值得实现的。论文地址：https://www.arxiv.org/pdf/1608.08021v3.pdfgithub项目:https://github.com/sanghoon/pva-faster-rcnn实现环境操作系统：Ubuntu 16.04LTS显卡：NVIDIA GTX 1080TICUDA8.0Cudnn V5.1其它配置忽略（不重要） Caffe General Dependencies安装 sudo apt-get update sudo apt-get upgrade sudo apt-get install -y build-essential cmake git pkg-config sudo apt-get install -y libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install -y libatlas-base-dev sudo apt-get install -y --no-install-recommends libboost-all-dev sudo apt-get install -y libgflags-dev libgoogle-glog-dev liblmdb-dev (Python general) sudo apt-get install -y python-pip \#(Python 2.7 development files) sudo apt-get install -y python-dev sudo apt-get install -y python-numpy python-scipy OpenCV安装此处根据git master branch安装的3.2.0-dev版本 \#Build tools: sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev cd ~ git clone https://github.com/opencv/opencv.git cd opencv mkdir build cd build cmake -D CMAKE_BUILD_TYPE=RELEASE \ -D CMAKE_INSTALL_PREFIX=/usr/local \ -D WITH_CUDA=ON \ -D WITH_CUBLAS=1 \ -D INSTALL_PYTHON_EXAMPLES=ON \ -D BUILD_EXAMPLES=ON .. make all -j16（此处根据你自己的计算机性能进行安装） sudo make -j16 install sudo ldconfig 检查安装 $ python \>>>import cv2 \>>>cv2.__version__ 输出：'3.2.0-dev' 有关CUDA 8.0+Cudnn5.1的安装请自行百度 pva-faster-rcnn的搭建 1.获取项目 git clone --recursive https://github.com/sanghoon/pva-faster-rcnn.git 2.编译建立Cython模块 安装python依赖 sudo pip install Cython sudo pip install easydict cd pva-faster-rcnn/lib 此处需要修改lib下的setup.py第135行 GPU 计算能力查看地址'nvcc': ['-arch=sm_35', https://developer.nvidia.com/cuda-gpus ![](http://upload-images.jianshu.io/upload_images/3478042-7250cd061ba5e39a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 比如我的是1080TI，计算能力为6.1，则修改为 'nvcc': ['-arch=sm_61', 执行 make all -j16 进行编译 3.编译建立Caffe和pycaffe cd pva-faster-rcnn/caffe-fast-rcnn cp Makefile.config.example Makefile.config gedit Makefile.config 修改Makefile.config文件: 需要修改的行如下 USE_CUDNN := 1 OPENCV_VERSION := 3 #CUDA directory contains bin/ and lib/ directories that we need. CUDA_DIR := /usr/local/cuda CUDA_DIR := /usr/local/cuda-8.0 # Whatever else you find you need goes here. INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-Linux-gnu/hdf5/serial/ INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-Linux-gnu/hdf5/serial/ #Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 #Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) USE_PKG_CONFIG := 1 Makefile修改 cd ~/pva-faster-rcnn/caffe-faster-rcnn gedit Makefile 需要注意的是暂时不要采用最新的cudnn v6版本，我使用的时候发觉会出错，v5.1版本修改此配置。 原有 NVCCFLAGS += -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) 修改为 NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) 更新caffe-fast-rcnn的caffe部分，因为此项目caffe不是最新涉及到cudnn的计算会报错。 # Make sure to clone with --recursive cd ~/pva-faster-rcnn/caffe-fast-rcnn git remote add -f caffe https://github.com/BVLC/caffe.git git merge -X theirs caffe/master gedit include/caffe/layers/python_layer.hpp \# Remove self_.attr("phase") = static_cast(this->phase_); cd ~/pva-faster-rcnn/caffe-faster-rcnn/python for req in $(cat requirements.txt); do sudo -H pip install $req --upgrade; done 编译安装 mkdir build cd build cmake .. make -j16 all make -j16 pycaffe make -j16 install Caffe路径设置 gedit ~/.bashrc export CAFFE_ROOT=~/py-faster-rcnn/caffe-fast-rcnn export PYTHONPATH=~/py-faster-rcnn/caffe-fast-rcnn/python:$PYTHONPATH source ~/.bashrc 检查安装 $ python >>> import caffe >>> caffe.__version__ sudo -H pip install easydict sudo apt-get install python-gi-cairo python-tk 4.下载预训练模型如果有VPN网速好的话直接采用pva-faster-rcnn/models下的的shell脚本进行下载此处采用百度网盘下载链接：http://pan.baidu.com/s/1kVRRPDd 密码：1cdt1、打开文件将test.model放入$pva-faster-rcnn/models/pvanet/full/这个目录下2、将test(1).model重命名为test.model放入$pva-faster-rcnn/models/pvanet/comp/目录下5.voc2007数据集下载打开终端（任何目录）输入： wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar解压（严格按照此顺序）： tar xvf VOCtrainval_06-Nov-2007.tartar xvf VOCtest_06-Nov-2007.tartar xvf VOCdevkit_08-Jun-2007.tar将生成的VOCdevkit文件夹更名为VOCdevkit2007移动至$pva-faster-rcnn/data/目录下面6.测试需要注意的是缺少test.pt，请将pvanet_obsolete下的对应文件复制过来即可终端输入： cd $pva-faster-rcnn1、full/test.model测试： ./tools/test_net.py –gpu 0 –def models/pvanet/full/test.pt –net models/pvanet/full/test.model –cfg models/pvanet/cfgs/submit_0716.yml2、Comp/test.model测试： ./tools/test_net.py –gpu 0 –def models/pvanet/comp/test.pt –net models/pvanet/comp/test.model –cfg models/pvanet/cfgs/submit_0716.yml此测试会得到系列类别的AP值3.模型的可视化检测demo此处需要重新编写demo.py文件可在我github下fork的分支获取该文件demo.py文件https://github.com/YgRen/pva-faster-rcnn cd pva-faster-rcnn执行 ./tools/demo.py –gpu 0 –def models/pvanet/comp/test.pt –net models/pvanet/comp/test.model生成可视化结果 记录：关于自定义数据集的训练和调参后期会进行发布，关于VOC07和12数据集的训练，直接参照github项目地址操作即可，最近会更新论文精读。]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>目标检测</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
